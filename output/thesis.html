<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <meta http-equiv="Content-Style-Type" content="text/css" />
        <meta name="generator" content="pandoc" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
                    <meta name="author" content="James Owers" />
                        <title>Evaluating Algorithms that Learn how to Compose Music from Scratch</title>
        <style type="text/css">code{white-space: pre;}</style>
                                                    <style>
                                                    body {
                                                        font-family: Georgia;
                                                        max-width: 800px;
                                                        margin: 0 auto;
                                                        line-height: 30px;
                                                        font-size: 18px;
                                                        padding-left: 350px;
                                                        padding-right: 50px;
                                                        color: #111;
                                                    }

                                                    h1, h2, h3, h4, h5, h6 {
                                                        font-family: Arial;
                                                    }

                                                    h1 {
                                                        padding-top: 200px;
                                                        line-height: 50px;
                                                    }
                                                    h2 {
                                                        padding-top: 30px;
                                                    }
                                                    h3 {
                                                        padding-top: 20px;
                                                    }
                                                    h4 {
                                                        padding-top: 10px;
                                                    }
                                                    p {
                                                        text-align: justify;
                                                    }
                                                    p a {
                                                        word-wrap: break-word;
                                                        white-space: pre;
                                                    }
                                                    code {
                                                        word-wrap: break-word;
                                                    }
                                                    blockquote {
                                                        border-left: 3px solid #eee;
                                                        margin-left: 20px;
                                                        padding-left: 20px;
                                                    }

                                                    ::selection {
                                                        background-color: #E4E4E4;
                                                    }

                                                    table {
                                                        width: 100%;
                                                    }
                                                    table caption {
                                                        font-weight: bold;
                                                    }
                                                    table tr {
                                                        padding: 0;
                                                        margin: 0;
                                                        background-color: #f0f0f0;
                                                    }
                                                    table tr.even {
                                                        background-color: #fafafa;
                                                    }
                                                    table td {
                                                        margin: 0;
                                                        padding: 3px 5px;
                                                    }

                                                    p span.added {
                                                        color: green;
                                                        background-color: #FFF3C5;
                                                    }
                                                    p span.removed {
                                                        color: red;
                                                        background-color: #FFF3C5;
                                                    }

                                                    #title-page {
                                                        padding: 80px 0;
                                                    }

                                                    #TOC {
                                                        position: fixed;
                                                        left: 0;
                                                        top: 0;
                                                        overflow-y: scroll;
                                                        height: 100%;
                                                        background: #fafafa;
                                                        max-width: 300px;
                                                        font-family: Arial;
                                                        font-size: 15px;
                                                        line-height: 30px;
                                                    }
                                                    ::-webkit-scrollbar {
                                                        width: 8px;
                                                    }
                                                    ::-webkit-scrollbar-track {
                                                        background-color: #ECECEC;
                                                    }
                                                    ::-webkit-scrollbar-thumb {
                                                        background-color: #B0B0B0;
                                                        border-radius: 8px;
                                                    }
                                                    #TOC > ul {
                                                        padding-right: 10px;
                                                    }
                                                    #TOC ul {
                                                        list-style: none;
                                                        padding-left: 20px;
                                                    }
                                                    #TOC ul li a {
                                                        text-decoration: none;
                                                        color: #364149;
                                                        text-overflow: ellipsis;
                                                        display: block;
                                                        white-space: nowrap;
                                                        overflow: hidden;
                                                    }
                                                    #TOC ul li a:hover {
                                                        color: #008cff;
                                                    }

                                                    .figure {
                                                        text-align: center;
                                                    }
                                                    .figure p {
                                                        text-align: center;
                                                        font-style: italic;
                                                    }
                                                    .figure img {
                                                          width: 100%;
                                                    }
                                                    </style>
                <script src="js/jquery.js"></script>
        <script src="js/diff.js"></script>
        <script src="js/main.js"></script>
    </head>
    <body>
        
                    <div id="header">
                <h1 class="title">Evaluating Algorithms that Learn how to Compose Music from Scratch</h1>
                                <h1 class="subtitle">New long– and short–term metrics for evaluating model-generated compositions</h1>
                                                <h2 class="author">James Owers</h2>
                                                <h3 class="date">January 2021</h3>
                            </div>
        
                    <div id="TOC">
                <ul>
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#acknowledgements">Acknowledgements</a></li>
                <li><a href="#abbreviations">Abbreviations</a></li>
                <li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a>
                <ul>
                <li><a href="#target-problems-addressed-in-this-thesis"><span class="toc-section-number">1.1</span> Target problems addressed in this thesis</a></li>
                <li><a href="#historical-background"><span class="toc-section-number">1.2</span> Historical background</a>
                <ul>
                <li><a href="#first-instance-of-music-generation"><span class="toc-section-number">1.2.1</span> First instance of music generation</a></li>
                <li><a href="#mozart-using-mechanical-aids-for-idea-generation"><span class="toc-section-number">1.2.2</span> Mozart using mechanical aids for idea generation</a></li>
                <li><a href="#ada-lovelace-noting-computers-could-generate-music"><span class="toc-section-number">1.2.3</span> Ada lovelace noting computers could generate music</a></li>
                </ul></li>
                <li><a href="#modern-interest-and-achievements"><span class="toc-section-number">1.3</span> Modern interest and achievements</a></li>
                <li><a href="#what-are-algorithms-that-learn"><span class="toc-section-number">1.4</span> What are algorithms that learn</a></li>
                <li><a href="#what-is-composing-music"><span class="toc-section-number">1.5</span> What is composing music</a></li>
                <li><a href="#what-does-it-mean-to-compose-from-scratch"><span class="toc-section-number">1.6</span> What does it mean to compose from scratch</a></li>
                <li><a href="#motivation-for-this-work"><span class="toc-section-number">1.7</span> Motivation for this work</a></li>
                <li><a href="#scope-of-this-work"><span class="toc-section-number">1.8</span> Scope of this work</a></li>
                <li><a href="#list-of-contributions-in-this-thesis"><span class="toc-section-number">1.9</span> List of contributions in this thesis</a></li>
                </ul></li>
                <li><a href="#literature-review"><span class="toc-section-number">2</span> Literature Review</a>
                <ul>
                <li><a href="#challenges-addressed-in-the-literature-and-how-they-are-evaluated"><span class="toc-section-number">2.1</span> Challenges addressed in the literature and how they are evaluated</a></li>
                <li><a href="#a-summary-of-evaluation-methods-for-creative-models"><span class="toc-section-number">2.2</span> A summary of evaluation methods for creative models</a>
                <ul>
                <li><a href="#the-need-for-automated-metrics"><span class="toc-section-number">2.2.1</span> The need for automated metrics</a></li>
                <li><a href="#differences-between-evaluating-audio-and-symbolic-outputs"><span class="toc-section-number">2.2.2</span> Differences between evaluating audio and symbolic outputs</a></li>
                <li><a href="#the-impossible-task-of-satisfying-all-evaluation-requirements-with-a-single-metric"><span class="toc-section-number">2.2.3</span> The impossible task of satisfying all evaluation requirements with a single metric</a></li>
                <li><a href="#evaluation-metrics-and-representations-used-for-extracting-musical-structures"><span class="toc-section-number">2.2.4</span> Evaluation metrics and representations used for extracting musical structures</a></li>
                </ul></li>
                <li><a href="#models-for-composing-music"><span class="toc-section-number">2.3</span> Models for composing music</a>
                <ul>
                <li><a href="#models-which-learn-from-scratch"><span class="toc-section-number">2.3.1</span> Models which learn from scratch</a></li>
                <li><a href="#models-which-do-not-learn-from-scratch"><span class="toc-section-number">2.3.2</span> Models which do not learn from scratch</a></li>
                </ul></li>
                <li><a href="#methods-for-representing-music-on-a-computer"><span class="toc-section-number">2.4</span> Methods for representing music on a computer</a>
                <ul>
                <li><a href="#information-that-must-be-captured-about-a-musical-performance"><span class="toc-section-number">2.4.1</span> Information that must be captured about a musical performance</a></li>
                <li><a href="#the-different-representations-of-symbolic-musical-information"><span class="toc-section-number">2.4.2</span> The different representations of symbolic musical information</a></li>
                <li><a href="#availability-of-data-for-each-representation"><span class="toc-section-number">2.4.3</span> Availability of data for each representation</a></li>
                <li><a href="#availability-of-software-for-different-representations"><span class="toc-section-number">2.4.4</span> Availability of software for different representations</a></li>
                <li><a href="#evidence-from-the-literature-regarding-modelling-performance-differences"><span class="toc-section-number">2.4.5</span> Evidence from the literature regarding modelling performance differences</a></li>
                </ul></li>
                <li><a href="#ethical-considerations-when-designing-automated-methods-for-composing-music"><span class="toc-section-number">2.5</span> Ethical considerations when designing automated methods for composing music</a></li>
                </ul></li>
                <li><a href="#new-metrics-for-evaluating-musical-generations"><span class="toc-section-number">3</span> New metrics for Evaluating Musical Generations</a>
                <ul>
                <li><a href="#features-of-symbolic-music"><span class="toc-section-number">3.1</span> Features of symbolic music</a></li>
                <li><a href="#the-midi-degradation-toolkit"><span class="toc-section-number">3.2</span> The MIDI degradation toolkit</a></li>
                <li><a href="#a-phrase-level-metric-for-short-term-structure"><span class="toc-section-number">3.3</span> A phrase-level metric for short-term structure</a></li>
                <li><a href="#a-piece-level-metric-for-long-term-structure"><span class="toc-section-number">3.4</span> A piece-level metric for long-term structure</a></li>
                </ul></li>
                <li><a href="#evaluating-state-of-the-art-music-generating-models"><span class="toc-section-number">4</span> Evaluating State-of-the-Art Music-generating Models</a>
                <ul>
                <li><a href="#comparative-analysis-using-new-and-existing-metrics"><span class="toc-section-number">4.1</span> Comparative analysis using new and existing metrics</a></li>
                <li><a href="#strengths-and-shortcomings-of-existing-models"><span class="toc-section-number">4.2</span> Strengths and shortcomings of existing models</a></li>
                <li><a href="#avenues-for-improvement"><span class="toc-section-number">4.3</span> Avenues for improvement</a></li>
                </ul></li>
                <li><a href="#a-new-model"><span class="toc-section-number">5</span> A New Model</a></li>
                <li><a href="#conclusion"><span class="toc-section-number">6</span> Conclusion</a></li>
                <li><a href="#references"><span class="toc-section-number">7</span> References</a></li>
                </ul>
            </div>
                                <!-- This page is for an official declaration. -->
                                <p>    </p>
                                <h1 class="unnumbered" id="abstract">Abstract</h1>
                                <!-- This is the abstract -->
                                <p>Evaluating whether creative content generated by a computer is ‘good,’ be it music, images, or text, is unsolved and not even well defined. We identify a property of music which is not modelled well, and propose new evaluation metrics for music generation which can be used to distinguish between real and generated data, and thus be useful for automatic quantitative analysis of generation quality.</p>
                                <p>We focus on symbolic music because …TODO… This is interesting because …TODO… and it has implications for …TODO…</p>
                                <p>Finally, we make recommendations for how to make progress with respect to music generation and related tasks.</p>
                                <h1 class="unnumbered" id="acknowledgements">Acknowledgements</h1>
                                <!-- This is for acknowledging all of the people who helped out -->
                                <p>Interdum et malesuada fames ac ante ipsum primis in faucibus. Aliquam congue fermentum ante, semper porta nisl consectetur ut. Duis ornare sit amet dui ac faucibus. Phasellus ullamcorper leo vitae arcu ultricies cursus. Duis tristique lacus eget metus bibendum, at dapibus ante malesuada. In dictum nulla nec porta varius. Fusce et elit eget sapien fringilla maximus in sit amet dui.</p>
                                <p>Mauris eget blandit nisi, faucibus imperdiet odio. Suspendisse blandit dolor sed tellus venenatis, venenatis fringilla turpis pretium. Donec pharetra arcu vitae euismod tincidunt. Morbi ut turpis volutpat, ultrices felis non, finibus justo. Proin convallis accumsan sem ac vulputate. Sed rhoncus ipsum eu urna placerat, sed rhoncus erat facilisis. Praesent vitae vestibulum dui. Proin interdum tellus ac velit varius, sed finibus turpis placerat.</p>
                                <!-- Use the \newpage command to force a new page -->
                                <!-- 
                                The \listoffigures will use short captions first, and the whole caption if none is
                                present. To keep this list readable, ensure each figure has a short caption, e.g.
                                ![main_text_caption](source/figures/my_image.pdf "short caption used in alt text and
                                \listoffigures"){#fig:mylabel}{ width=50% }

                                See chapter 4 for more examples.
                                -->
                                <h1 class="unnumbered" id="abbreviations">Abbreviations</h1>

                                <h1 data-number="1" id="introduction"><span class="header-section-number">1</span> Introduction</h1>
                                <h2 data-number="1.1" id="target-problems-addressed-in-this-thesis"><span class="header-section-number">1.1</span> Target problems addressed in this thesis</h2>
                                <h2 data-number="1.2" id="historical-background"><span class="header-section-number">1.2</span> Historical background</h2>
                                <p>…TODO… Give historical background</p>
                                <h3 data-number="1.2.1" id="first-instance-of-music-generation"><span class="header-section-number">1.2.1</span> First instance of music generation</h3>
                                <p>…TODO… From Section 1.2 <span class="citation" data-cites="briot_deep_2019">(Briot et al. 2019)</span></p>
                                <blockquote>
                                <p>The first music generated by computer appeared in 1957. It was a 17 seconds long melody named “The Silver Scale” by its author Newman Guttman and was generated by a software for sound synthesis named Music I, developed by Mathews at Bell Laboratories</p>
                                </blockquote>
                                <h3 data-number="1.2.2" id="mozart-using-mechanical-aids-for-idea-generation"><span class="header-section-number">1.2.2</span> Mozart using mechanical aids for idea generation</h3>
                                <p>…TODO… From footnote 7 in Section 1.2 <span class="citation" data-cites="briot_deep_2019">(Briot et al. 2019)</span></p>
                                <blockquote>
                                <p>One of the first documented case of stochastic music, long before computers, is the Musikalisches Wurfelspiel (Dice Music) by Wolfgang Amadeus Mozart. It was designed for using dice to generate music by concatenating randomly selected predefined music segments composed in a given style (Austrian waltz in a given key).</p>
                                </blockquote>
                                <h3 data-number="1.2.3" id="ada-lovelace-noting-computers-could-generate-music"><span class="header-section-number">1.2.3</span> Ada lovelace noting computers could generate music</h3>
                                <p>…TODO… From <span class="citation" data-cites="hollings_ada_2018">(Hollings et al. 2018)</span> Ada Lovelace, “Sketch of the Analytical Engine invented by Charles Babbage, Esq., by L. F. Menabrea,” Scientific Memoirs, vol. 3, ed. Richard Taylor, 1843, pp. 666-731 (this quote on p 694).</p>
                                <blockquote>
                                <p>“Note G” is the culmination of Lovelace’s paper, following many pages of detailed explanation of the operation of the Engine and the cards, and of the notation of the tables. The paper shows Lovelace’s obsessive attention to mathematical details - it also shows her imagination in thinking about the bigger picture.</p>
                                </blockquote>
                                <blockquote>
                                <p>Lovelace overseed a fundamental principle of the machine, that the operations, defined by the cards, are separate from the data and the results. She observed that the machine might act upon things other than numbers, if those things satisfied mathematical rules.</p>
                                </blockquote>
                                <blockquote>
                                <p>Supposing that the fundamental relations of pitched sounds in the science of harmony and of musical composition were susceptible of such expression and adaptations, the engine might compose elaborate and scientific pieces of music of any degree of complexity or extent.</p>
                                </blockquote>
                                <p>Lovelace also has the Lovelace Test of Creativity attributed to her - see <span class="citation" data-cites="ariza_interrogator_2009">(Ariza 2009)</span>.</p>
                                <h2 data-number="1.3" id="modern-interest-and-achievements"><span class="header-section-number">1.3</span> Modern interest and achievements</h2>
                                <p>…TODO…</p>
                                <ul>
                                <li><a href="https://www.bbc.co.uk/news/av/technology-52236563">Imogen Heap: How AI is helping to push music creativity</a></li>
                                <li><a href="https://www.vprobroadcast.com/titles/ai-songcontest.html">The AI Song Contest</a> - In the AI ​​Song Contest teams of musicians, artists, scientists and developers take on the challenge of creating a new Eurovision-like hit with the help of artificial intelligence.</li>
                                <li><a href="https://www.bbc.co.uk/programmes/m000cngg">Swooshes, Seaboards, Synths and Spawn</a></li>
                                <li><a href="https://overcast.fm/+S_7no2kwM">David Rosen and Scott Miles on the Neuroscience of Music and Creativity</a></li>
                                <li>AI Music Generation Challenge 2020 - <span class="citation" data-cites="sturm_ai_2020">(Sturm 2020)</span></li>
                                </ul>
                                <h2 data-number="1.4" id="what-are-algorithms-that-learn"><span class="header-section-number">1.4</span> What are algorithms that learn</h2>
                                <p>…TODO… define/introduce machine learning</p>
                                <h2 data-number="1.5" id="what-is-composing-music"><span class="header-section-number">1.5</span> What is composing music</h2>
                                <p>…TODO…</p>
                                <h2 data-number="1.6" id="what-does-it-mean-to-compose-from-scratch"><span class="header-section-number">1.6</span> What does it mean to compose from scratch</h2>
                                <p>…TODO… what is the minimum information we supply as a starting point? What feedback do we give?</p>
                                <h2 data-number="1.7" id="motivation-for-this-work"><span class="header-section-number">1.7</span> Motivation for this work</h2>
                                <p>…TODO…</p>
                                <ul>
                                <li>why are we focussing on metrics and not human evaluation
                                <ul>
                                <li>how do we benchmark without them?</li>
                                </ul></li>
                                <li>where is the gap - not many metrics are available</li>
                                <li>why do we care about ‘from scratch’</li>
                                </ul>
                                <p><span class="citation" data-cites="sturm_benchmarking_2017">(Sturm 2017)</span> gives background as to why we need metrics but no specific methods.</p>
                                <p>Why do we need computational rather than human analysis</p>
                                <p><span class="citation" data-cites="marsden_music_2016">(Marsden 2016)</span></p>
                                <blockquote>
                                <p>Computational music analysis needs to carve out a place for itself where it is not simply mimicry of human analysis, but a place which is not so distant from the human activity to prevent useful communication with musicians. We need to recall the potential value of computational analysis, the reasons we embark on this enterprise at all.</p>
                                </blockquote>
                                <p>Metrics - more audio features than symbolic.</p>
                                <p><span class="citation" data-cites="giraud_chapter_2015">(Giraud et al. 2015)</span></p>
                                <blockquote>
                                <p>There is less work to date that focuses on segmentation of symbolic scores</p>
                                </blockquote>
                                <h2 data-number="1.8" id="scope-of-this-work"><span class="header-section-number">1.8</span> Scope of this work</h2>
                                <p>…TODO… Symbolic music only</p>
                                <h2 data-number="1.9" id="list-of-contributions-in-this-thesis"><span class="header-section-number">1.9</span> List of contributions in this thesis</h2>
                                <p>…TODO…</p>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                We provide a literature review of the current state-of-the-art with respect to algorithmic music composition: the challenges addressed and models presented<ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Note that <span class="citation" data-cites="briot_deep_2019">(Briot et al. 2019)</span> does not include <code>COCOnet</code> nor any Transformer models so details of these is a new contribution</li>
                                </ul></li>
                                <li><input type="checkbox" disabled="" />
                                Consistent open source python re-implementations of all models compared</li>
                                </ul>
                                <h1 data-number="2" id="literature-review"><span class="header-section-number">2</span> Literature Review</h1>
                                <p>…TODO…</p>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Add content and TODOs from Quip:<ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                https://quip.com/vVjVADMamfDm/2-Literature-Review</li>
                                <li><input type="checkbox" disabled="" />
                                https://quip.com/v0MOAQGvMH3O/Literature-Review-Org-Notes</li>
                                <li><input type="checkbox" disabled="" />
                                move data and notes tables in ./tables (use YAML, <a href="https://stackoverflow.com/a/3790497/2550114">allows large text blocks for notes</a> easy to convert to table with python)</li>
                                </ul></li>
                                <li><input type="checkbox" disabled="" />
                                outline the different problems people currently try to / can solve, and how these problems relate to ‘being able to compose’</li>
                                <li><input type="checkbox" disabled="" />
                                Review available metrics<ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Motivate the need for automated evaluation metrics</li>
                                <li><input type="checkbox" disabled="" />
                                Why has there been more work on audio than symbolic?</li>
                                <li><input type="checkbox" disabled="" />
                                Motivate the need for better evaluation for both short and long term by highlighting shortcomings for each method reviewed</li>
                                </ul></li>
                                <li><input type="checkbox" disabled="" />
                                Describe state-of-the-art generative models for music composition</li>
                                <li><input type="checkbox" disabled="" />
                                Identify a gap with respect to modelling long term dependencies by outlining claims and proof of them thus far - this is a specific thing we are going show is poorly evaluated</li>
                                <li><input type="checkbox" disabled="" />
                                Inform the reader about the multitude of different ways we can represent music and their relative strengths and weaknesses</li>
                                <li><input type="checkbox" disabled="" />
                                Address ethical shortcomings with respect to learning to compose</li>
                                <li><input type="checkbox" disabled="" />
                                Update bibtex references to non-arxiv reference if available</li>
                                </ul>
                                <h2 data-number="2.1" id="challenges-addressed-in-the-literature-and-how-they-are-evaluated"><span class="header-section-number">2.1</span> Challenges addressed in the literature and how they are evaluated</h2>
                                <h2 data-number="2.2" id="a-summary-of-evaluation-methods-for-creative-models"><span class="header-section-number">2.2</span> A summary of evaluation methods for creative models</h2>
                                <p>…TODO… how to evaluate generative models with a focus on music - how do people evaluate their success</p>
                                <h3 data-number="2.2.1" id="the-need-for-automated-metrics"><span class="header-section-number">2.2.1</span> The need for automated metrics</h3>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Humans are expensive - show some efforts</li>
                                <li><input type="checkbox" disabled="" />
                                Humans do not agree - show some research proving this</li>
                                <li><input type="checkbox" disabled="" />
                                Humans are susceptible to change their opinion depending on context - show some research proving this</li>
                                <li><input type="checkbox" disabled="" />
                                Consistency is key when tracking performance over the long term</li>
                                <li><input type="checkbox" disabled="" />
                                Attempts to unify metrics and human opinion - give WMT as an example <span class="citation" data-cites="haddow_wmt_2020">(Haddow 2020)</span></li>
                                </ul>
                                <h3 data-number="2.2.2" id="differences-between-evaluating-audio-and-symbolic-outputs"><span class="header-section-number">2.2.2</span> Differences between evaluating audio and symbolic outputs</h3>
                                <p><span class="citation" data-cites="dhariwal_jukebox_2020">(Dhariwal et al. 2020)</span></p>
                                <h3 data-number="2.2.3" id="the-impossible-task-of-satisfying-all-evaluation-requirements-with-a-single-metric"><span class="header-section-number">2.2.3</span> The impossible task of satisfying all evaluation requirements with a single metric</h3>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Should tie a metric with performance for an intended task <span class="citation" data-cites="theis_note_2016">(Theis et al. 2016)</span></li>
                                </ul>
                                <h3 data-number="2.2.4" id="evaluation-metrics-and-representations-used-for-extracting-musical-structures"><span class="header-section-number">2.2.4</span> Evaluation metrics and representations used for extracting musical structures</h3>
                                <h2 data-number="2.3" id="models-for-composing-music"><span class="header-section-number">2.3</span> Models for composing music</h2>
                                <p>…TODO… State caveats about our distinctions:</p>
                                <ol type="1">
                                <li>Learning is essentially copying</li>
                                <li>By specifying the method of learning, we are incorporating expert knowledge</li>
                                <li>All models must work with a human composer to some extent - the programmer must choose a representation for the music and is therefore a composer in some senses</li>
                                </ol>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Make and curate comparison table of models:<ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                keep as csv</li>
                                <li><input type="checkbox" disabled="" />
                                at min, we can use pandas to read and auto convert for insertion here</li>
                                <li><input type="checkbox" disabled="" />
                                is there a way to <code>@reference</code> the file here and have pandoc insert? &lt;- <strong>do not spend time on this, cursory google!</strong></li>
                                </ul></li>
                                <li><input type="checkbox" disabled="" />
                                Music Transformer <span class="citation" data-cites="huang_music_2019">(Huang et al. 2019)</span></li>
                                <li><input type="checkbox" disabled="" />
                                MuseNet <span class="citation" data-cites="payne_musenet_2019">(Payne 2019)</span></li>
                                <li><input type="checkbox" disabled="" />
                                Extend table from Chapter 7 and information from Chapter 6 in review paper <span class="citation" data-cites="briot_deep_2019">(Briot et al. 2019)</span></li>
                                <li><input type="checkbox" disabled="" />
                                Go through https://paperswithcode.com/task/music-generation</li>
                                </ul>
                                <h3 data-number="2.3.1" id="models-which-learn-from-scratch"><span class="header-section-number">2.3.1</span> Models which learn from scratch</h3>
                                <p>…TODO… These are the models which our research pertains to</p>
                                <h3 data-number="2.3.2" id="models-which-do-not-learn-from-scratch"><span class="header-section-number">2.3.2</span> Models which do not learn from scratch</h3>
                                <p>…TODO… These models are stated to highlight why they are different, have an unfair advantage in certain contexts, or explain why they are out of scope with respect to the investigation of this thesis.</p>
                                <h4 data-number="2.3.2.1" id="heuristic-models-which-primarily-copy-and-edit-music-from-a-database"><span class="header-section-number">2.3.2.1</span> Heuristic models which primarily copy and edit music from a database</h4>
                                <h4 data-number="2.3.2.2" id="models-which-incorporate-expert-knowledge-into-their-design"><span class="header-section-number">2.3.2.2</span> Models which incorporate expert knowledge into their design</h4>
                                <p>…TODO… e.g. with respect to structural hierarchy</p>
                                <h4 data-number="2.3.2.3" id="models-which-only-work-in-conjunction-with-a-human-composer"><span class="header-section-number">2.3.2.3</span> Models which only work in conjunction with a human composer</h4>
                                <h4 data-number="2.3.2.4" id="proprietary-models"><span class="header-section-number">2.3.2.4</span> Proprietary models</h4>
                                <p>Models for which adequate details of their design are not publicly available</p>
                                <h2 data-number="2.4" id="methods-for-representing-music-on-a-computer"><span class="header-section-number">2.4</span> Methods for representing music on a computer</h2>
                                <p>…TODO… how to represent music data - (in relation to ‘from scratch,’ what is the minimal information supplied to the models, and is there evidence of what difference it makes (either by experiment or just by reasoning?)</p>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Note our desires with respect to our modelling challenges: we want the input to be <em>minimal and flexible</em> - the model should learn as much as possible as if it were a human listener<ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Ideally we would work directly on sound, but this involves an additional layer of representation.</li>
                                </ul></li>
                                </ul>
                                <h3 data-number="2.4.1" id="information-that-must-be-captured-about-a-musical-performance"><span class="header-section-number">2.4.1</span> Information that must be captured about a musical performance</h3>
                                <h3 data-number="2.4.2" id="the-different-representations-of-symbolic-musical-information"><span class="header-section-number">2.4.2</span> The different representations of symbolic musical information</h3>
                                <h4 data-number="2.4.2.1" id="summary-of-differences"><span class="header-section-number">2.4.2.1</span> Summary of differences</h4>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Highlight where <em>information</em> captured by each representation is both <strong>different</strong> and <strong>more/less amenable to being learned</strong></li>
                                </ul>
                                <h3 data-number="2.4.3" id="availability-of-data-for-each-representation"><span class="header-section-number">2.4.3</span> Availability of data for each representation</h3>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Quantity,</li>
                                <li><input type="checkbox" disabled="" />
                                Quality,</li>
                                <li><input type="checkbox" disabled="" />
                                Legal issues</li>
                                </ul>
                                <h3 data-number="2.4.4" id="availability-of-software-for-different-representations"><span class="header-section-number">2.4.4</span> Availability of software for different representations</h3>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Describe MusPy <span class="citation" data-cites="dong_muspy_2020">(Dong et al. 2020)</span> for conversion between data formats</li>
                                <li><input type="checkbox" disabled="" />
                                Describe Music21 <span class="citation" data-cites="cuthbert_music21_2010">(Cuthbert &amp; Ariza 2010)</span> for conversion between data formats</li>
                                </ul>
                                <h3 data-number="2.4.5" id="evidence-from-the-literature-regarding-modelling-performance-differences"><span class="header-section-number">2.4.5</span> Evidence from the literature regarding modelling performance differences</h3>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Find any reviews (or lack thereof) of model performance differences with respect to:<ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                evaluation metrics</li>
                                <li><input type="checkbox" disabled="" />
                                speed</li>
                                </ul></li>
                                </ul>
                                <h2 data-number="2.5" id="ethical-considerations-when-designing-automated-methods-for-composing-music"><span class="header-section-number">2.5</span> Ethical considerations when designing automated methods for composing music</h2>
                                <h1 data-number="3" id="new-metrics-for-evaluating-musical-generations"><span class="header-section-number">3</span> New metrics for Evaluating Musical Generations</h1>
                                <h2 data-number="3.1" id="features-of-symbolic-music"><span class="header-section-number">3.1</span> Features of symbolic music</h2>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                Discuss different features which can be extracted</li>
                                </ul>
                                <h2 data-number="3.2" id="the-midi-degradation-toolkit"><span class="header-section-number">3.2</span> The MIDI degradation toolkit</h2>
                                <p><span class="citation" data-cites="mcleod_midi_2020">(McLeod et al. 2020)</span></p>
                                <h2 data-number="3.3" id="a-phrase-level-metric-for-short-term-structure"><span class="header-section-number">3.3</span> A phrase-level metric for short-term structure</h2>
                                <h2 data-number="3.4" id="a-piece-level-metric-for-long-term-structure"><span class="header-section-number">3.4</span> A piece-level metric for long-term structure</h2>
                                <h1 data-number="4" id="evaluating-state-of-the-art-music-generating-models"><span class="header-section-number">4</span> Evaluating State-of-the-Art Music-generating Models</h1>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                If the main contribution is evaluating the long term structure, then ensure this is emphasized either in the title of this chapter or in the first lines</li>
                                </ul>
                                <h2 data-number="4.1" id="comparative-analysis-using-new-and-existing-metrics"><span class="header-section-number">4.1</span> Comparative analysis using new and existing metrics</h2>
                                <ul>
                                <li>Use phrase and piece level metrics to evaluate state-of-the-art models</li>
                                <li>Compare and contrast, outlining the issues identified (e.g. meandering, no high-level structure)</li>
                                </ul>
                                <h2 data-number="4.2" id="strengths-and-shortcomings-of-existing-models"><span class="header-section-number">4.2</span> Strengths and shortcomings of existing models</h2>
                                <h2 data-number="4.3" id="avenues-for-improvement"><span class="header-section-number">4.3</span> Avenues for improvement</h2>
                                <h1 data-number="5" id="a-new-model"><span class="header-section-number">5</span> A New Model</h1>
                                <p>Potential ideas:</p>
                                <ul>
                                <li>An improved generative model for music
                                <ul>
                                <li>Training like BERT? <a href="http://jalammar.github.io/illustrated-bert/" class="uri">http://jalammar.github.io/illustrated-bert/</a></li>
                                <li>Using mdtk for data augmentation in training (negative examples?), making them more robust</li>
                                <li>Alternative training objectives:
                                <ul>
                                <li>crossentropy slow and not musically informed</li>
                                <li>can we use something akin to word error rate (this has been done for text)</li>
                                </ul></li>
                                </ul></li>
                                <li>Alternative ways to encode music: encoding chords and phrases in a low-rank continuous space
                                <ul>
                                <li>Have done some work on this with convnets and generating continuations
                                <ul>
                                <li>low rank was enforced by cross-product ing two vecs</li>
                                </ul></li>
                                <li>Could investigate effect of different representations for music on performance</li>
                                </ul></li>
                                </ul>
                                <h1 data-number="6" id="conclusion"><span class="header-section-number">6</span> Conclusion</h1>
                                <!-- 
                                Do not edit this page.

                                References are automatically generated from the BibTex file (phd_thesis_references.bib)

                                ...which you should create using your reference manager.
                                -->
                                <h1 data-number="7" id="references"><span class="header-section-number">7</span> References</h1>
                                <p>…TODO…</p>
                                <ul class="task-list">
                                <li><input type="checkbox" disabled="" />
                                check over using https://www.cl.cam.ac.uk/~ga384/bibfix.html</li>
                                <li><input type="checkbox" disabled="" />
                                also check with https://github.com/yuchenlin/rebiber</li>
                                <li><input type="checkbox" disabled="" />
                                Check all title casing correct (use curly braces around letters which should remain as they are). All titles should be in Title Case.</li>
                                </ul>
                                <div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
                                <div id="ref-ariza_interrogator_2009" class="csl-entry" role="doc-biblioentry">
                                Ariza, C., 2009. The <span>Interrogator</span> as <span>Critic</span>: <span>The</span> <span>Turing</span> <span>Test</span> and the <span>Evaluation</span> of <span>Generative</span> <span>Music</span> <span>Systems</span>. <em>Computer Music Journal</em>, 33(2), pp.48–70. Available at: <a href="https://www.jstor.org/stable/40301027">https://www.jstor.org/stable/40301027</a> [Accessed January 22, 2021].
                                </div>
                                <div id="ref-briot_deep_2019" class="csl-entry" role="doc-biblioentry">
                                Briot, J.-P., Hadjeres, G. &amp; Pachet, F.-D., 2019. Deep <span>Learning</span> <span>Techniques</span> for <span>Music</span> <span>Generation</span> – <span>A</span> <span>Survey</span>. <em>arXiv:1709.01620 [cs]</em>. Available at: <a href="http://arxiv.org/abs/1709.01620">http://arxiv.org/abs/1709.01620</a> [Accessed January 22, 2021].
                                </div>
                                <div id="ref-cuthbert_music21_2010" class="csl-entry" role="doc-biblioentry">
                                Cuthbert, M.S. &amp; Ariza, C., 2010. music21: <span>A</span> <span>Toolkit</span> for <span>Computer</span>-<span>Aided</span> <span>Musicology</span> and <span>Symbolic</span> <span>Music</span> <span>Data</span>. In <em>Proceedings of the 11th <span>International</span> <span>Society</span> for <span>Music</span> <span>Information</span> <span>Retrieval</span> <span>Conference</span></em>. Available at: <a href="https://dspace.mit.edu/handle/1721.1/84963">https://dspace.mit.edu/handle/1721.1/84963</a> [Accessed January 23, 2021].
                                </div>
                                <div id="ref-dhariwal_jukebox_2020" class="csl-entry" role="doc-biblioentry">
                                Dhariwal, P. et al., 2020. Jukebox: <span>A</span> <span>Generative</span> <span>Model</span> for <span>Music</span>. <em>arXiv:2005.00341 [cs, eess, stat]</em>. Available at: <a href="http://arxiv.org/abs/2005.00341">http://arxiv.org/abs/2005.00341</a> [Accessed February 19, 2021].
                                </div>
                                <div id="ref-dong_muspy_2020" class="csl-entry" role="doc-biblioentry">
                                Dong, H.-W. et al., 2020. <span>MusPy</span>: <span>A</span> <span>Toolkit</span> for <span>Symbolic</span> <span>Music</span> <span>Generation</span>. In <em>Proceedings of the 21st <span>International</span> <span>Society</span> for <span>Music</span> <span>Information</span> <span>Retrieval</span> <span>Conference</span></em>. Montreal, Canada. Available at: <a href="https://program.ismir2020.net/static/final_papers/187.pdf">https://program.ismir2020.net/static/final_papers/187.pdf</a> [Accessed January 26, 2021].
                                </div>
                                <div id="ref-giraud_chapter_2015" class="csl-entry" role="doc-biblioentry">
                                Giraud, M., Groult, R. &amp; Levé, F., 2015. Chapter 5 <span>Computational</span> <span>Analysis</span> of <span>Musical</span> <span>Form</span>. Available at: <a href="https:///paper/Chapter-5-Computational-Analysis-of-Musical-Form-Giraud-Groult/800e6d00d48b57115bbf8c000a9f14b7048fdcf9">/paper/Chapter-5-Computational-Analysis-of-Musical-Form-Giraud-Groult/800e6d00d48b57115bbf8c000a9f14b7048fdcf9</a> [Accessed February 17, 2021].
                                </div>
                                <div id="ref-haddow_wmt_2020" class="csl-entry" role="doc-biblioentry">
                                Haddow, B., 2020. <span>EMNLP</span> 2020 <span>Fifth</span> <span>Conference</span> on <span>Machine</span> <span>Translation</span> (<span>WMT20</span>). <em>2020 Fifth Conference on Machine Translation (WMT20)</em>. Available at: <a href="http://www.statmt.org/wmt20/">http://www.statmt.org/wmt20/</a> [Accessed January 23, 2021].
                                </div>
                                <div id="ref-hollings_ada_2018" class="csl-entry" role="doc-biblioentry">
                                Hollings, C., Martin, U. &amp; Rice, A.C., 2018. <em>Ada <span>Lovelace</span>: <span>The</span> <span>Making</span> of a <span>Computer</span> <span>Scientist</span></em>, Oxford: Bodleian Library.
                                </div>
                                <div id="ref-huang_music_2019" class="csl-entry" role="doc-biblioentry">
                                Huang, C.-Z.A. et al., 2019. Music transformer: Generating music with long-term structure. In <em>7th international conference on learning representations, <span>ICLR</span> 2019, new orleans, LA, USA, may 6-9, 2019</em>. OpenReview.net. Available at: <a href="https://openreview.net/forum?id=rJe4ShAcF7">https://openreview.net/forum?id=rJe4ShAcF7</a>.
                                </div>
                                <div id="ref-marsden_music_2016" class="csl-entry" role="doc-biblioentry">
                                Marsden, A., 2016. Music <span>Analysis</span> by <span>Computer</span>: <span>Ontology</span> and <span>Epistemology</span>. In D. Meredith, ed. <em>Computational <span>Music</span> <span>Analysis</span></em>. Cham: Springer International Publishing, pp. 3–28. Available at: <a href="https://doi.org/10.1007/978-3-319-25931-4_1">https://doi.org/10.1007/978-3-319-25931-4_1</a> [Accessed February 17, 2021].
                                </div>
                                <div id="ref-mcleod_midi_2020" class="csl-entry" role="doc-biblioentry">
                                McLeod, A., Owers, J. &amp; Yoshii, K., 2020. The <span>MIDI</span> <span>Degradation</span> <span>Toolkit</span>: <span>Symbolic</span> <span>Music</span> <span>Augmentation</span> and <span>Correction</span>. In <em>Proceedings of the 21st <span>International</span> <span>Society</span> for <span>Music</span> <span>Information</span> <span>Retrieval</span> <span>Conference</span></em>. Montreal, Canada. Available at: <a href="https://program.ismir2020.net/static/final_papers/182.pdf">https://program.ismir2020.net/static/final_papers/182.pdf</a> [Accessed January 26, 2021].
                                </div>
                                <div id="ref-payne_musenet_2019" class="csl-entry" role="doc-biblioentry">
                                Payne, C., 2019. <span>MuseNet</span>. <em>OpenAI</em>. Available at: <a href="https://openai.com/blog/musenet/">https://openai.com/blog/musenet/</a> [Accessed January 23, 2021].
                                </div>
                                <div id="ref-sturm_ai_2020" class="csl-entry" role="doc-biblioentry">
                                Sturm, B., 2020. <span>AI</span> <span>Music</span> <span>Generation</span> <span>Challenge</span> 2020. <em>The 2020 Joint Conference on AI Music Creativity</em>. Available at: <a href="https://boblsturm.github.io/aimusic2020/">https://boblsturm.github.io/aimusic2020/</a> [Accessed January 22, 2021].
                                </div>
                                <div id="ref-sturm_benchmarking_2017" class="csl-entry" role="doc-biblioentry">
                                Sturm, B.L.T., 2017. Benchmarking <span>“music generation systems?”</span> <em>Folk the Algorithms</em>. Available at: <a href="https://highnoongmt.wordpress.com/2017/03/19/benchmarking-music-generation-systems/">https://highnoongmt.wordpress.com/2017/03/19/benchmarking-music-generation-systems/</a> [Accessed February 18, 2021].
                                </div>
                                <div id="ref-theis_note_2016" class="csl-entry" role="doc-biblioentry">
                                Theis, L., Oord, A. van den &amp; Bethge, M., 2016. A note on the evaluation of generative models. In Y. Bengio &amp; Y. LeCun, eds. <em>4th international conference on learning representations, <span>ICLR</span> 2016, san juan, puerto rico, may 2-4, 2016, conference track proceedings</em>. Available at: <a href="http://arxiv.org/abs/1511.01844">http://arxiv.org/abs/1511.01844</a>.
                                </div>
                                </div>
            </body>
</html>
