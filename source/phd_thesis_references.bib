
@article{briot_deep_2019,
	title = {Deep {Learning} {Techniques} for {Music} {Generation} -- {A} {Survey}},
	url = {http://arxiv.org/abs/1709.01620},
	abstract = {This paper is a survey and an analysis of different ways of using deep learning (deep artificial neural networks) to generate musical content. We propose a methodology based on five dimensions for our analysis: Objective - What musical content is to be generated? Examples are: melody, polyphony, accompaniment or counterpoint. - For what destination and for what use? To be performed by a human(s) (in the case of a musical score), or by a machine (in the case of an audio file). Representation - What are the concepts to be manipulated? Examples are: waveform, spectrogram, note, chord, meter and beat. - What format is to be used? Examples are: MIDI, piano roll or text. - How will the representation be encoded? Examples are: scalar, one-hot or many-hot. Architecture - What type(s) of deep neural network is (are) to be used? Examples are: feedforward network, recurrent network, autoencoder or generative adversarial networks. Challenge - What are the limitations and open challenges? Examples are: variability, interactivity and creativity. Strategy - How do we model and control the process of generation? Examples are: single-step feedforward, iterative feedforward, sampling or input manipulation. For each dimension, we conduct a comparative analysis of various models and techniques and we propose some tentative multidimensional typology. This typology is bottom-up, based on the analysis of many existing deep-learning based systems for music generation selected from the relevant literature. These systems are described and are used to exemplify the various choices of objective, representation, architecture, challenge and strategy. The last section includes some discussion and some prospects.},
	urldate = {2021-01-22},
	journal = {arXiv:1709.01620 [cs]},
	author = {Briot, Jean-Pierre and Hadjeres, Gaëtan and Pachet, François-David},
	month = aug,
	year = {2019},
	note = {arXiv: 1709.01620},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = { BriotArXiv2018 Deep Learning Techniques for Music Generation – A Survey 2.pdf:/Users/jfowers/Zotero/storage/BRWXCHBW/ BriotArXiv2018 Deep Learning Techniques for Music Generation – A Survey 2.pdf:application/pdf;arXiv Fulltext PDF:/Users/jfowers/Zotero/storage/DL9MALUN/Briot et al. - 2019 - Deep Learning Techniques for Music Generation -- A.pdf:application/pdf;arXiv.org Snapshot:/Users/jfowers/Zotero/storage/WDPGAKY9/1709.html:text/html},
}

@book{hollings_ada_2018,
	address = {Oxford},
	title = {Ada {Lovelace}: the making of a computer scientist},
	isbn = {978-1-85124-488-1},
	shorttitle = {Ada {Lovelace}},
	abstract = {Ada, Countess of Lovelace (1815-1852), daughter of romantic poet Lord Byron and his highly educated wife, Anne Isabella, is sometimes called the world's first computer programmer and has become an icon for women in technology. But how did a young woman in the nineteenth century, without access to formal school or university education, acquire the knowledge and expertise to become a pioneer of computer science? Although an unusual pursuit for women at the time, Ada Lovelace studied science and mathematics from a young age. This book uses previously unpublished archival material to explore her precocious childhood, from her ideas for a steam-powered flying horse to penetrating questions about the science of rainbows. A remarkable correspondence course with the eminent mathematician Augustus De Morgan shows her developing into a gifted, perceptive and knowledgeable mathematician. Active in Victorian London's social and scientific elite alongside Mary Somerville, Michael Faraday and Charles Dickens, Ada Lovelace became fascinated by the computing machines devised by Charles Babbage. The table of mathematical formulae sometimes called the `first programme' occurs in her paper about his most ambitious invention, his unbuilt `Analytical Engine'.0Ada Lovelace died at just thirty-six, but her paper still strikes a chord to this day, with clear explanations of the principles of computing, and broader ideas on computer music and artificial intelligence now realised in modern digital computers. Featuring images of the `first programme' and Lovelace's correspondence, alongside mathematical models, and contemporary illustrations, this book shows how Ada Lovelace, with astonishing prescience, explored key mathematical questions to understand the principles behind modern computing},
	publisher = {Bodleian Library},
	author = {Hollings, Christopher and Martin, Ursula and Rice, Adrian C.},
	year = {2018},
	keywords = {Biography, Computer programming, History, Lovelace, Ada King, Women computer programmers, Women in computer science, Women mathematicians},
}

@article{ariza_interrogator_2009,
	title = {The {Interrogator} as {Critic}: {The} {Turing} {Test} and the {Evaluation} of {Generative} {Music} {Systems}},
	volume = {33},
	issn = {0148-9267},
	shorttitle = {The {Interrogator} as {Critic}},
	url = {https://www.jstor.org/stable/40301027},
	number = {2},
	urldate = {2021-01-22},
	journal = {Computer Music Journal},
	author = {Ariza, Christopher},
	year = {2009},
	note = {Publisher: The MIT Press},
	pages = {48--70},
	file = {Ariza - 2009 - The Interrogator as Critic The Turing Test and th.pdf:/Users/jfowers/Zotero/storage/7P9A8ZRQ/Ariza - 2009 - The Interrogator as Critic The Turing Test and th.pdf:application/pdf},
}

@misc{sturm_2020_2020,
	title = {The 2020 {Joint} {Conference} on {AI} {Music} {Creativity}},
	url = {https://boblsturm.github.io/aimusic2020/},
	abstract = {The 2020 Joint Conference on AI Music Creativity},
	language = {en-US},
	urldate = {2021-01-22},
	journal = {The 2020 Joint Conference on AI Music Creativity},
	author = {Sturm, Bob},
	year = {2020},
	file = {Snapshot:/Users/jfowers/Zotero/storage/QDD6ZJYX/aimusic2020.html:text/html},
}

@article{theis_note_2016,
	title = {A note on the evaluation of generative models},
	url = {http://arxiv.org/abs/1511.01844},
	abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
	urldate = {2021-01-23},
	journal = {arXiv:1511.01844 [cs, stat]},
	author = {Theis, Lucas and Oord, Aäron van den and Bethge, Matthias},
	month = apr,
	year = {2016},
	note = {arXiv: 1511.01844},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jfowers/Zotero/storage/YET2NG64/Theis et al. - 2016 - A note on the evaluation of generative models.pdf:application/pdf;arXiv.org Snapshot:/Users/jfowers/Zotero/storage/K4K2KVMP/1511.html:text/html},
}

@misc{website_wmt_2020,
	title = {2020 {Fifth} {Conference} on {Machine} {Translation} ({WMT20})},
	url = {http://www.statmt.org/wmt20/},
	urldate = {2021-01-23},
	year = {2020},
	file = {2020 Fifth Conference on Machine Translation (WMT20):/Users/jfowers/Zotero/storage/FRMG4F29/wmt20.html:text/html},
}

@article{dong_muspy_2020,
	title = {{MusPy}: {A} {Toolkit} for {Symbolic} {Music} {Generation}},
	shorttitle = {{MusPy}},
	url = {http://arxiv.org/abs/2008.01951},
	abstract = {In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocessing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others---a process which is made easier by MusPy's dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at https://github.com/salu133445/muspy .},
	urldate = {2021-01-23},
	journal = {arXiv:2008.01951 [cs, eess, stat]},
	author = {Dong, Hao-Wen and Chen, Ke and McAuley, Julian and Berg-Kirkpatrick, Taylor},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.01951},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jfowers/Zotero/storage/MQZTQPHX/Dong et al. - 2020 - MusPy A Toolkit for Symbolic Music Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/jfowers/Zotero/storage/T4L2XLHE/2008.html:text/html},
}

@article{cuthbert_music21_2010,
	title = {music21: {A} {Toolkit} for {Computer}-{Aided} {Musicology} and {Symbolic} {Music} {Data}},
	copyright = {Creative Commons Attribution-Noncommercial-Share Alike 3.0},
	shorttitle = {music21},
	url = {https://dspace.mit.edu/handle/1721.1/84963},
	abstract = {Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (score- based) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to provide powerful software tools integrated with sophisticated musical knowledge to both musicians with little programming experience (especially musicologists) and to programmers with only modest music theory skills. This paper introduces the music21 system, demonstrating how to use it and the types of problems it is well suited toward advancing. We include numerous examples of its power and flexibility, including demonstrations of graphing data and generating annotated musical scores.},
	language = {en\_US},
	urldate = {2021-01-23},
	journal = {Michael Cuthbert},
	author = {Cuthbert, Michael Scott and Ariza, Christopher},
	month = aug,
	year = {2010},
	note = {Accepted: 2014-02-14T18:40:17Z
ISBN: 9789039353813
Publisher: International Society for Music Information Retrieval},
	file = {Full Text PDF:/Users/jfowers/Zotero/storage/6QF6WXSN/Cuthbert and Ariza - 2010 - music21 A Toolkit for Computer-Aided Musicology a.pdf:application/pdf;Snapshot:/Users/jfowers/Zotero/storage/F47XVBE4/84963.html:text/html},
}
