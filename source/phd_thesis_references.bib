@article{antoniou_data_2018,
 abstract = {Conditional GANs trained to generate data augmented samples of their conditional inputs used to enhance vanilla classification and one shot learning systems such as matching networks and pixel...},
 author = {Antoniou, Anthreas and Storkey, Amos and Edwards, Harrison},
 file = {Snapshot:/Users/james.owers/Zotero/storage/FU4LEINN/forum.html:text/html;Full Text PDF:/Users/james.owers/Zotero/storage/QIMRD5M8/Antoniou et al. - 2018 - Data Augmentation Generative Adversarial Networks.pdf:application/pdf},
 language = {en},
 title = {Data {Augmentation} {Generative} {Adversarial} {Networks}},
 url = {https://openreview.net/forum?id=S1Auv-WRZ},
 urldate = {2021-03-10},
 year = {2018}
}

@article{ariza_interrogator_2009,
 author = {Ariza, Christopher},
 file = {Ariza - 2009 - The Interrogator as Critic The Turing Test and th.pdf:/Users/james.owers/Zotero/storage/7P9A8ZRQ/Ariza - 2009 - The Interrogator as Critic The Turing Test and th.pdf:application/pdf},
 issn = {0148-9267},
 journal = {Computer Music Journal},
 note = {Publisher: The MIT Press},
 number = {2},
 pages = {48--70},
 shorttitle = {critic},
 title = {The {Interrogator} as {Critic}: {The} {Turing} {Test} and the {Evaluation} of {Generative} {Music} {Systems}},
 url = {https://www.jstor.org/stable/40301027},
 urldate = {2021-01-22},
 volume = {33},
 year = {2009}
}

@article{briot_deep_2019,
 abstract = {This paper is a survey and an analysis of different ways of using deep learning (deep artificial neural networks) to generate musical content. We propose a methodology based on five dimensions for our analysis: Objective - What musical content is to be generated? Examples are: melody, polyphony, accompaniment or counterpoint. - For what destination and for what use? To be performed by a human(s) (in the case of a musical score), or by a machine (in the case of an audio file). Representation - What are the concepts to be manipulated? Examples are: waveform, spectrogram, note, chord, meter and beat. - What format is to be used? Examples are: MIDI, piano roll or text. - How will the representation be encoded? Examples are: scalar, one-hot or many-hot. Architecture - What type(s) of deep neural network is (are) to be used? Examples are: feedforward network, recurrent network, autoencoder or generative adversarial networks. Challenge - What are the limitations and open challenges? Examples are: variability, interactivity and creativity. Strategy - How do we model and control the process of generation? Examples are: single-step feedforward, iterative feedforward, sampling or input manipulation. For each dimension, we conduct a comparative analysis of various models and techniques and we propose some tentative multidimensional typology. This typology is bottom-up, based on the analysis of many existing deep-learning based systems for music generation selected from the relevant literature. These systems are described and are used to exemplify the various choices of objective, representation, architecture, challenge and strategy. The last section includes some discussion and some prospects.},
 annote = {Comment: 209 pages. This paper is a simplified version of the book: J.-P. Briot, G. Hadjeres and F.-D. Pachet, Deep Learning Techniques for Music Generation, Computational Synthesis and Creative Systems, Springer, 2019},
 author = {Briot, Jean-Pierre and Hadjeres, Gaëtan and Pachet, François-David},
 file = { BriotArXiv2018 Deep Learning Techniques for Music Generation – A Survey 2.pdf:/Users/james.owers/Zotero/storage/BRWXCHBW/ BriotArXiv2018 Deep Learning Techniques for Music Generation – A Survey 2.pdf:application/pdf;arXiv.org Snapshot:/Users/james.owers/Zotero/storage/WDPGAKY9/1709.html:text/html;arXiv Fulltext PDF:/Users/james.owers/Zotero/storage/DL9MALUN/Briot et al. - 2019 - Deep Learning Techniques for Music Generation -- A.pdf:application/pdf},
 journal = {arXiv:1709.01620 [cs]},
 keywords = {Computer Science - Machine Learning, Computer Science - Sound},
 note = {arXiv: 1709.01620},
 shorttitle = {dl4music-survey},
 title = {Deep {Learning} {Techniques} for {Music} {Generation} -- {A} {Survey}},
 url = {http://arxiv.org/abs/1709.01620},
 urldate = {2021-01-22},
 year = {2019}
}

@inproceedings{cubuk_autoaugment_2019,
 author = {Ekin D. Cubuk and
Barret Zoph and
Dandelion Man{\'{e}} and
Vijay Vasudevan and
Quoc V. Le},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/CubukZMVL19.bib},
 booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
2019, Long Beach, CA, USA, June 16-20, 2019},
 doi = {10.1109/CVPR.2019.00020},
 pages = {113--123},
 publisher = {Computer Vision Foundation / {IEEE}},
 timestamp = {Tue, 17 Nov 2020 00:00:00 +0100},
 title = {AutoAugment: Learning Augmentation Strategies From Data},
 url = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Cubuk\_AutoAugment\_Learning\_Augmentation\_Strategies\_From\_Data\_CVPR\_2019\_paper.html},
 year = {2019}
}

@inproceedings{cuthbert_music21_2010,
 abstract = {Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (score- based) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to provide powerful software tools integrated with sophisticated musical knowledge to both musicians with little programming experience (especially musicologists) and to programmers with only modest music theory skills. This paper introduces the music21 system, demonstrating how to use it and the types of problems it is well suited toward advancing. We include numerous examples of its power and flexibility, including demonstrations of graphing data and generating annotated musical scores.},
 author = {Cuthbert, Michael Scott and Ariza, Christopher},
 booktitle = {Proceedings of the 11th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
 copyright = {Creative Commons Attribution-Noncommercial-Share Alike 3.0},
 file = {Snapshot:/Users/james.owers/Zotero/storage/F47XVBE4/84963.html:text/html;Full Text PDF:/Users/james.owers/Zotero/storage/6QF6WXSN/Cuthbert and Ariza - 2010 - music21 A Toolkit for Computer-Aided Musicology a.pdf:application/pdf},
 language = {en\_US},
 note = {Accepted: 2014-02-14T18:40:17Z
ISBN: 9789039353813
Publisher: International Society for Music Information Retrieval},
 shorttitle = {music21},
 title = {music21: {A} {Toolkit} for {Computer}-{Aided} {Musicology} and {Symbolic} {Music} {Data}},
 url = {https://dspace.mit.edu/handle/1721.1/84963},
 urldate = {2021-01-23},
 year = {2010}
}

@inproceedings{deng_imagenet_2009,
 author = {Jia Deng and
Wei Dong and
Richard Socher and
Li{-}Jia Li and
Kai Li and
Fei{-}Fei Li},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/DengDSLL009.bib},
 booktitle = {2009 {IEEE} Computer Society Conference on Computer Vision and Pattern
Recognition {(CVPR} 2009), 20-25 June 2009, Miami, Florida, {USA}},
 doi = {10.1109/CVPR.2009.5206848},
 pages = {248--255},
 publisher = {{IEEE} Computer Society},
 timestamp = {Fri, 27 Mar 2020 00:00:00 +0100},
 title = {ImageNet: {A} large-scale hierarchical image database},
 url = {https://doi.org/10.1109/CVPR.2009.5206848},
 year = {2009}
}

@misc{dhariwal_jukebox_2020,
 abstract = {We’re introducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We’re releasing the model weights and code, along with a tool to explore the generated samples.},
 author = {Dhariwal, Prafulla* and Jun, Heewoo* and Payne, Christine* and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
 file = {Snapshot:/Users/james.owers/Zotero/storage/J8QEK43M/jukebox.html:text/html},
 journal = {OpenAI},
 language = {en},
 title = {Jukebox},
 url = {https://openai.com/blog/jukebox/},
 urldate = {2021-02-19},
 year = {2020}
}

@article{dhariwal_jukebox_2020-1,
 abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
 author = {Dhariwal, Prafulla* and Jun, Heewoo* and Payne, Christine* and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
 file = {arXiv Fulltext PDF:/Users/james.owers/Zotero/storage/2FXBVPHZ/Dhariwal et al. - 2020 - Jukebox A Generative Model for Music.pdf:application/pdf;arXiv.org Snapshot:/Users/james.owers/Zotero/storage/HWBR4YJX/2005.html:text/html},
 journal = {arXiv:2005.00341 [cs, eess, stat]},
 keywords = {Computer Science - Machine Learning, Computer Science - Sound, Statistics - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing},
 note = {arXiv: 2005.00341},
 shorttitle = {Jukebox},
 title = {Jukebox: {A} {Generative} {Model} for {Music}},
 url = {http://arxiv.org/abs/2005.00341},
 urldate = {2021-02-19},
 year = {2020}
}

@inproceedings{dong_muspy_2020,
 abstract = {In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocessing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others—a process which is made easier by MusPy’s dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at https://github.com/salu133445/muspy.},
 address = {Montreal, Canada},
 author = {Dong, Hao-Wen and Chen, Ke and McAuley, Julian and Berg-Kirkpatrick, Taylor},
 booktitle = {Proceedings of the 21st {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
 file = {Dong et al. - 2020 - MUSPY A TOOLKIT FOR SYMBOLIC MUSIC GENERATION.pdf:/Users/james.owers/Zotero/storage/RIMBD8HS/Dong et al. - 2020 - MUSPY A TOOLKIT FOR SYMBOLIC MUSIC GENERATION.pdf:application/pdf},
 language = {en},
 shorttitle = {muspy},
 title = {{MusPy}: {A} {Toolkit} for {Symbolic} {Music} {Generation}},
 url = {https://program.ismir2020.net/static/final_papers/187.pdf},
 urldate = {2021-01-26},
 year = {2020}
}

@incollection{giraud_computational_2016,
 abstract = {Can a computer understand musical forms? Musical forms describe how a piece of music is structured. They explain how the sections work together through repetition, contrast, and variation: repetition brings unity, and variation brings interest. Learning how to hear, to analyse, to play, or even to write music in various forms is part of music education. In this chapter, we briefly review some theories of musical form, and discuss the challenges of computational analysis of musical form. We discuss two sets of problems, segmentation and form analysis. We present studies in music information retrieval (MIR) related to both problems. Thinking about codification and automatic analysis of musical forms will help the development of better MIR algorithms.},
 address = {Cham},
 author = {Giraud, Mathieu and Groult, Richard and Levé, Florence},
 booktitle = {Computational {Music} {Analysis}},
 doi = {10.1007/978-3-319-25931-4_5},
 editor = {Meredith, David},
 isbn = {978-3-319-25931-4},
 keywords = {Music Information Retrieval, Music Structure, Music Theorist, Pitch Interval, Reference Analysis},
 language = {en},
 pages = {113--136},
 publisher = {Springer International Publishing},
 title = {Computational {Analysis} of {Musical} {Form}},
 url = {https://doi.org/10.1007/978-3-319-25931-4_5},
 urldate = {2021-02-21},
 year = {2016}
}

@misc{haddow_wmt_2020,
 annote = {Loïc Barrault (University of Sheffield)Ondřej Bojar (Charles University in Prague)Fethi Bougares (University of Le Mans)Rajen Chatterjee (Apple)Marta R. Costa-jussà (Universitat Politècnica de CatalunyaChristian Federmann (MSR)Mark Fishel (University of Tartu)Alexander Fraser (LMU Munich)Yvette Graham (DCU)Romann Grundkiewicz (MSR)Paco Guzman (Facebook)Barry Haddow (University of Edinburgh)Matthias Huck (LMU Munich)Antonio Jimeno Yepes (IBM Research Australia)Tom Kocmi (MSR)Philipp Koehn (University of Edinburgh / Johns Hopkins University)André Martins (Unbabel)Makoto Morishita (NTT)Christof Monz (University of Amsterdam)Masaaki Nagata (NTT)Toshiaki Nakazawa (University of Tokyo)Matteo Negri (FBK)Aurélie Névéol (LIMSI, CNRS)Mariana Neves (German Federal Institute for Risk Assessment)Martin Popel (Charles University in Prague)Matt Post (Johns Hopkins University)Marco Turchi (FBK)Marcos Zampieri (Rochester Institute of Technology)},
 author = {Haddow, Barry},
 file = {2020 Fifth Conference on Machine Translation (WMT20):/Users/james.owers/Zotero/storage/FRMG4F29/wmt20.html:text/html},
 journal = {2020 Fifth Conference on Machine Translation (WMT20)},
 shorttitle = {wmt2020},
 title = {{EMNLP} 2020 {Fifth} {Conference} on {Machine} {Translation} ({WMT20})},
 url = {http://www.statmt.org/wmt20/},
 urldate = {2021-01-23},
 year = {2020}
}

@book{hollings_ada_2018,
 abstract = {Ada, Countess of Lovelace (1815-1852), daughter of romantic poet Lord Byron and his highly educated wife, Anne Isabella, is sometimes called the world's first computer programmer and has become an icon for women in technology. But how did a young woman in the nineteenth century, without access to formal school or university education, acquire the knowledge and expertise to become a pioneer of computer science? Although an unusual pursuit for women at the time, Ada Lovelace studied science and mathematics from a young age. This book uses previously unpublished archival material to explore her precocious childhood, from her ideas for a steam-powered flying horse to penetrating questions about the science of rainbows. A remarkable correspondence course with the eminent mathematician Augustus De Morgan shows her developing into a gifted, perceptive and knowledgeable mathematician. Active in Victorian London's social and scientific elite alongside Mary Somerville, Michael Faraday and Charles Dickens, Ada Lovelace became fascinated by the computing machines devised by Charles Babbage. The table of mathematical formulae sometimes called the `first programme' occurs in her paper about his most ambitious invention, his unbuilt `Analytical Engine'.0Ada Lovelace died at just thirty-six, but her paper still strikes a chord to this day, with clear explanations of the principles of computing, and broader ideas on computer music and artificial intelligence now realised in modern digital computers. Featuring images of the `first programme' and Lovelace's correspondence, alongside mathematical models, and contemporary illustrations, this book shows how Ada Lovelace, with astonishing prescience, explored key mathematical questions to understand the principles behind modern computing},
 address = {Oxford},
 author = {Hollings, Christopher and Martin, Ursula and Rice, Adrian C.},
 isbn = {978-1-85124-488-1},
 keywords = {Biography, Computer programming, History, Lovelace, Ada King, Women computer programmers, Women in computer science, Women mathematicians},
 publisher = {Bodleian Library},
 shorttitle = {making-cs},
 title = {Ada {Lovelace}: {The} {Making} of a {Computer} {Scientist}},
 year = {2018}
}

@inproceedings{huang_music_2019,
 author = {Cheng{-}Zhi Anna Huang and
Ashish Vaswani and
Jakob Uszkoreit and
Ian Simon and
Curtis Hawthorne and
Noam Shazeer and
Andrew M. Dai and
Matthew D. Hoffman and
Monica Dinculescu and
Douglas Eck},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/HuangVUSHSDHDE19.bib},
 booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
New Orleans, LA, USA, May 6-9, 2019},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {Music Transformer: Generating Music with Long-Term Structure},
 url = {https://openreview.net/forum?id=rJe4ShAcF7},
 year = {2019}
}

@inproceedings{johnson_generating_2017,
 abstract = {We describe a neural network architecture which enables prediction and composition of polyphonic music in a manner that preserves translation-invariance of the dataset. Specifically, we demonstrate training a probabilistic model of polyphonic music using a set of parallel, tied-weight recurrent networks, inspired by the structure of convolutional neural networks. This model is designed to be invariant to transpositions, but otherwise is intentionally given minimal information about the musical domain, and tasked with discovering patterns present in the source dataset. We present two versions of the model, denoted TP-LSTM-NADE and BALSTM, and also give methods for training the network and for generating novel music. This approach attains high performance at a musical prediction task and successfully creates note sequences which possess measure-level musical structure.},
 address = {Cham},
 author = {Johnson, Daniel D.},
 booktitle = {Computational {Intelligence} in {Music}, {Sound}, {Art} and {Design}},
 doi = {10.1007/978-3-319-55750-2_9},
 editor = {Correia, João and Ciesielski, Vic and Liapis, Antonios},
 isbn = {978-3-319-55750-2},
 keywords = {Convolutional Neural Network, Joint Probability Distribution, Prediction Task, Recurrent Neural Network, Translation Invariance},
 language = {en},
 pages = {128--143},
 publisher = {Springer International Publishing},
 series = {Lecture {Notes} in {Computer} {Science}},
 shorttitle = {tied-paranets},
 title = {Generating {Polyphonic} {Music} {Using} {Tied} {Parallel} {Networks}},
 year = {2017}
}

@inproceedings{lewis_bart_2020,
 address = {Online},
 author = {Lewis, Mike  and
Liu, Yinhan  and
Goyal, Naman  and
Ghazvininejad, Marjan  and
Mohamed, Abdelrahman  and
Levy, Omer  and
Stoyanov, Veselin  and
Zettlemoyer, Luke},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.703},
 pages = {7871--7880},
 publisher = {Association for Computational Linguistics},
 title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
 url = {https://www.aclweb.org/anthology/2020.acl-main.703},
 year = {2020}
}

@misc{lu_speller100_2021,
 abstract = {Researchers in Microsoft Search and AI created Speller100 to expand spelling correction technology to 100-plus languages. Learn how it uses the concept of language families and zero-shot learning to make spelling correction more inclusive of languages worldwide.},
 author = {Lu, Jingwen and Long, Jidong and Majumder, Rangan},
 file = {Snapshot:/Users/james.owers/Zotero/storage/N9XZYYH8/speller100-zero-shot-spelling-correction-at-scale-for-100-plus-languages.html:text/html},
 journal = {Microsoft Research},
 language = {en-US},
 title = {Speller100 expands spelling correction technology to 100+ languages},
 url = {https://www.microsoft.com/en-us/research/blog/speller100-zero-shot-spelling-correction-at-scale-for-100-plus-languages/},
 urldate = {2021-03-07},
 year = {2021}
}

@incollection{marsden_music_2016,
 abstract = {This chapter examines questions of what is to be analysed in computational music analysis, what is to be produced, and how one can have confidence in the results. These are not new issues for music analysis, but their consequences are here considered explicitly from the perspective of computational analysis. Music analysis without computers is able to operate with multiple or even indistinct conceptions of the material to be analysed because it can use multiple references whose meanings shift from context to context. Computational analysis, by contrast, must operate with definite inputs and produce definite outputs. Computational analysts must therefore face the issues of error and approximation explicitly. While computational analysis must retain contact with music analysis as it is generally practised, I argue that the most promising approach for the development of computational analysis is not systems to mimic human analysis, but instead systems to answer specific music-analytical questions. The chapter concludes with several consequent recommendations for future directions in computational music analysis.},
 address = {Cham},
 author = {Marsden, Alan},
 booktitle = {Computational {Music} {Analysis}},
 doi = {10.1007/978-3-319-25931-4_1},
 editor = {Meredith, David},
 isbn = {978-3-319-25931-4},
 keywords = {Computational Analysis, Kolmogorov Complexity, Music Information Retrieval, Music Notation, Music Perception},
 language = {en},
 pages = {3--28},
 publisher = {Springer International Publishing},
 shorttitle = {Music {Analysis} by {Computer}},
 title = {Music {Analysis} by {Computer}: {Ontology} and {Epistemology}},
 url = {https://doi.org/10.1007/978-3-319-25931-4_1},
 urldate = {2021-02-17},
 year = {2016}
}

@inproceedings{mckinney_data_2010,
 abstract = {In this paper we are concerned with the practical issues of working with data sets common to ﬁnance, statistics, and other related ﬁelds. pandas is a new library which aims to facilitate working with these data sets and to provide a set of fundamental building blocks for implementing statistical models. We will discuss speciﬁc design issues encountered in the course of developing pandas with relevant examples and some comparisons with the R language. We conclude by discussing possible future directions for statistical computing and data analysis using Python.},
 address = {Austin, Texas},
 author = {McKinney, Wes},
 doi = {10.25080/Majora-92bf1922-00a},
 file = {McKinney - 2010 - Data Structures for Statistical Computing in Pytho.pdf:/Users/james.owers/Zotero/storage/G4CKWWIA/McKinney - 2010 - Data Structures for Statistical Computing in Pytho.pdf:application/pdf},
 language = {en},
 pages = {56--61},
 title = {Data {Structures} for {Statistical} {Computing} in {Python}},
 url = {https://conference.scipy.org/proceedings/scipy2010/mckinney.html},
 urldate = {2021-03-10},
 year = {2010}
}

@inproceedings{mcleod_midi_2020,
 abstract = {In this paper, we introduce the MIDI Degradation Toolkit (MDTK), containing functions which take as input a musical excerpt (a set of notes with pitch, onset time, and duration), and return a “degraded” version of that excerpt with some error (or errors) introduced. Using the toolkit, we create the Altered and Corrupted MIDI Excerpts dataset version 1.0 (ACME v1.0), and propose four tasks of increasing difﬁculty to detect, classify, locate, and correct the degradations. We hypothesize that models trained for these tasks can be useful in (for example) improving automatic music transcription performance if applied as a post-processing step. To that end, MDTK includes a script that measures the distribution of different types of errors in a transcription, and creates a degraded dataset with similar properties. MDTK’s degradations can also be applied dynamically to a dataset during training (with or without the above script), generating novel degraded excerpts each epoch. MDTK could also be used to test the robustness of any system designed to take MIDI (or similar) data as input (e.g. systems designed for voice separation, metrical alignment, or chord detection) to such transcription errors or otherwise noisy data. The toolkit and dataset are both publicly available online, and we encourage contribution and feedback from the community.},
 address = {Montreal, Canada},
 author = {McLeod, Andrew and Owers, James and Yoshii, Kazuyoshi},
 booktitle = {Proceedings of the 21st {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
 file = {McLeod et al. - 2020 - THE MIDI DEGRADATION TOOLKIT SYMBOLIC MUSIC AUGME.pdf:/Users/james.owers/Zotero/storage/AB7XKFB3/McLeod et al. - 2020 - THE MIDI DEGRADATION TOOLKIT SYMBOLIC MUSIC AUGME.pdf:application/pdf},
 language = {en},
 shorttitle = {mdtk},
 title = {The {MIDI} {Degradation} {Toolkit}: {Symbolic} {Music} {Augmentation} and {Correction}},
 url = {https://program.ismir2020.net/static/final_papers/182.pdf},
 urldate = {2021-01-26},
 year = {2020}
}

@misc{pandas_software_2021,
 abstract = {This is a patch release in the 1.2.x series and includes some regression fixes. We recommend that all users upgrade to this version. See the full whatsnew for a list of all the changes. The release will be available on the defaults and conda-forge channels: conda install pandas Or via PyPI: python3 -m pip install --upgrade pandas Please report any issues with the release on the pandas issue tracker.},
 author = {{The pandas development team}},
 doi = {10.5281/zenodo.4572994},
 file = {Zenodo Snapshot:/Users/james.owers/Zotero/storage/EUIAFJJX/4572994.html:text/html},
 publisher = {Zenodo},
 shorttitle = {pandas-dev/pandas},
 title = {pandas-dev/pandas: {Pandas} 1.2.3},
 url = {https://zenodo.org/record/4572994#.YEhrWV37Rb8},
 urldate = {2021-03-10},
 year = {2021}
}

@misc{payne_musenet_2019,
 abstract = {We’ve created Musenet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments and can combine styles from country to Mozart to the Beatles.},
 author = {Payne, Christine},
 file = {Snapshot:/Users/james.owers/Zotero/storage/HRJK9MMB/musenet.html:text/html},
 journal = {OpenAI},
 language = {en},
 shorttitle = {musenet},
 title = {{MuseNet}},
 url = {https://openai.com/blog/musenet/},
 urldate = {2021-01-23},
 year = {2019}
}

@article{salamon_deep_2017,
 abstract = {The ability of deep convolutional neural networks (CNNs) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep CNN architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a “shallow” dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.},
 author = {Salamon, J. and Bello, J. P.},
 doi = {10.1109/LSP.2017.2657381},
 file = {IEEE Xplore Abstract Record:/Users/james.owers/Zotero/storage/LDP4GHZW/7829341.html:text/html},
 issn = {1558-2361},
 journal = {IEEE Signal Processing Letters},
 keywords = {audio data augmentation, audio signal processing, augmented training set, class-conditional data augmentation, Convolution, Data models, deep convolutional neural networks, Deep convolutional neural networks (CNNs), deep learning, discriminative spectro-temporal patterns, environmental sound classification, high-capacity models, neural nets, Neural networks, Shape, signal classification, Time-frequency analysis, Training, Training data, urban sound dataset},
 note = {Conference Name: IEEE Signal Processing Letters},
 number = {3},
 pages = {279--283},
 title = {Deep {Convolutional} {Neural} {Networks} and {Data} {Augmentation} for {Environmental} {Sound} {Classification}},
 volume = {24},
 year = {2017}
}

@misc{sturm_ai_2020,
 abstract = {The 2020 Joint Conference on AI Music Creativity},
 author = {Sturm, Bob},
 file = {Snapshot:/Users/james.owers/Zotero/storage/QDD6ZJYX/aimusic2020.html:text/html},
 journal = {The 2020 Joint Conference on AI Music Creativity},
 language = {en-US},
 shorttitle = {ai-mus-gen},
 title = {{AI} {Music} {Generation} {Challenge} 2020},
 url = {https://boblsturm.github.io/aimusic2020/},
 urldate = {2021-01-22},
 year = {2020}
}

@misc{sturm_benchmarking_2017,
 abstract = {A recent conversation on the Google magenta project is about benchmarking “music generation systems” like David Cope’s Experiments in Music Intelligence, magenta’s own recur…},
 author = {Sturm, Bob L. T.},
 file = {Snapshot:/Users/james.owers/Zotero/storage/2EZUFWII/benchmarking-music-generation-systems.html:text/html},
 journal = {Folk the Algorithms},
 language = {en},
 title = {Benchmarking “music generation systems”?},
 url = {https://highnoongmt.wordpress.com/2017/03/19/benchmarking-music-generation-systems/},
 urldate = {2021-02-18},
 year = {2017}
}

@inproceedings{theis_note_2016,
 author = {Lucas Theis and
A{\"{a}}ron van den Oord and
Matthias Bethge},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/TheisOB15.bib},
 booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
 editor = {Yoshua Bengio and
Yann LeCun},
 timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
 title = {A note on the evaluation of generative models},
 url = {http://arxiv.org/abs/1511.01844},
 year = {2016}
}

@article{van_rossum_python_1995,
 abstract = {Python is a simple, yet powerful programming language that bridges the gap between C and shell programming, and is thus ideally suited for ``throw-away programming'' and rapid prototyping. Its syntax is put together from constructs borrowed from a variety of other languages; most prominent are influences from ABC, C, Modula-3 and Icon. The Python interpreter is easily extended with new functions and data types implemented in C. Python is also suitable as an extension language for highly customizable C applications such as editors or window managers. Python is available for various operating systems, amongst which several flavors of UNIX, Amoeba, the Apple Macintosh O.S., and MS-DOS. This tutorial introduces the reader informally to the basic concepts and features of the Python language and system. It helps to have a Python interpreter handy for hands-on experience, but as the examples are self-contained, the tutorial can be read off-line as well. For a description of standard objects and modules, see the Python Library Reference manual. The Python Reference Manual gives a more formal definition of the language.},
 author = {van Rossum, Guido},
 file = {Snapshot:/Users/james.owers/Zotero/storage/FKZGHB3X/5007.html:text/html;Full Text PDF:/Users/james.owers/Zotero/storage/95Z95K7N/van Rossum - 1995 - Python tutorial.pdf:application/pdf},
 language = {en},
 note = {Number: R 9526},
 title = {Python tutorial},
 url = {https://ir.cwi.nl/pub/5007},
 urldate = {2021-02-21},
 year = {1995}
}

