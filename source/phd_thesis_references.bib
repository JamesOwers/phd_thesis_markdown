
@article{briot_deep_2019,
	title = {Deep {Learning} {Techniques} for {Music} {Generation} -- {A} {Survey}},
	shorttitle = {dl4music-survey},
	url = {http://arxiv.org/abs/1709.01620},
	abstract = {This paper is a survey and an analysis of different ways of using deep learning (deep artificial neural networks) to generate musical content. We propose a methodology based on five dimensions for our analysis: Objective - What musical content is to be generated? Examples are: melody, polyphony, accompaniment or counterpoint. - For what destination and for what use? To be performed by a human(s) (in the case of a musical score), or by a machine (in the case of an audio file). Representation - What are the concepts to be manipulated? Examples are: waveform, spectrogram, note, chord, meter and beat. - What format is to be used? Examples are: MIDI, piano roll or text. - How will the representation be encoded? Examples are: scalar, one-hot or many-hot. Architecture - What type(s) of deep neural network is (are) to be used? Examples are: feedforward network, recurrent network, autoencoder or generative adversarial networks. Challenge - What are the limitations and open challenges? Examples are: variability, interactivity and creativity. Strategy - How do we model and control the process of generation? Examples are: single-step feedforward, iterative feedforward, sampling or input manipulation. For each dimension, we conduct a comparative analysis of various models and techniques and we propose some tentative multidimensional typology. This typology is bottom-up, based on the analysis of many existing deep-learning based systems for music generation selected from the relevant literature. These systems are described and are used to exemplify the various choices of objective, representation, architecture, challenge and strategy. The last section includes some discussion and some prospects.},
	urldate = {2021-01-22},
	journal = {arXiv:1709.01620 [cs]},
	author = {Briot, Jean-Pierre and Hadjeres, Gaëtan and Pachet, François-David},
	month = aug,
	year = {2019},
	note = {arXiv: 1709.01620},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = { BriotArXiv2018 Deep Learning Techniques for Music Generation – A Survey 2.pdf:/Users/jfowers/Zotero/storage/BRWXCHBW/ BriotArXiv2018 Deep Learning Techniques for Music Generation – A Survey 2.pdf:application/pdf;arXiv Fulltext PDF:/Users/jfowers/Zotero/storage/DL9MALUN/Briot et al. - 2019 - Deep Learning Techniques for Music Generation -- A.pdf:application/pdf;arXiv.org Snapshot:/Users/jfowers/Zotero/storage/WDPGAKY9/1709.html:text/html},
}

@book{hollings_ada_2018,
	address = {Oxford},
	title = {Ada {Lovelace}: {The} {Making} of a {Computer} {Scientist}},
	isbn = {978-1-85124-488-1},
	shorttitle = {making-cs},
	abstract = {Ada, Countess of Lovelace (1815-1852), daughter of romantic poet Lord Byron and his highly educated wife, Anne Isabella, is sometimes called the world's first computer programmer and has become an icon for women in technology. But how did a young woman in the nineteenth century, without access to formal school or university education, acquire the knowledge and expertise to become a pioneer of computer science? Although an unusual pursuit for women at the time, Ada Lovelace studied science and mathematics from a young age. This book uses previously unpublished archival material to explore her precocious childhood, from her ideas for a steam-powered flying horse to penetrating questions about the science of rainbows. A remarkable correspondence course with the eminent mathematician Augustus De Morgan shows her developing into a gifted, perceptive and knowledgeable mathematician. Active in Victorian London's social and scientific elite alongside Mary Somerville, Michael Faraday and Charles Dickens, Ada Lovelace became fascinated by the computing machines devised by Charles Babbage. The table of mathematical formulae sometimes called the `first programme' occurs in her paper about his most ambitious invention, his unbuilt `Analytical Engine'.0Ada Lovelace died at just thirty-six, but her paper still strikes a chord to this day, with clear explanations of the principles of computing, and broader ideas on computer music and artificial intelligence now realised in modern digital computers. Featuring images of the `first programme' and Lovelace's correspondence, alongside mathematical models, and contemporary illustrations, this book shows how Ada Lovelace, with astonishing prescience, explored key mathematical questions to understand the principles behind modern computing},
	publisher = {Bodleian Library},
	author = {Hollings, Christopher and Martin, Ursula and Rice, Adrian C.},
	year = {2018},
	keywords = {Biography, Computer programming, History, Lovelace, Ada King, Women computer programmers, Women in computer science, Women mathematicians},
}

@article{ariza_interrogator_2009,
	title = {The {Interrogator} as {Critic}: {The} {Turing} {Test} and the {Evaluation} of {Generative} {Music} {Systems}},
	volume = {33},
	issn = {0148-9267},
	shorttitle = {critic},
	url = {https://www.jstor.org/stable/40301027},
	number = {2},
	urldate = {2021-01-22},
	journal = {Computer Music Journal},
	author = {Ariza, Christopher},
	year = {2009},
	note = {Publisher: The MIT Press},
	pages = {48--70},
	file = {Ariza - 2009 - The Interrogator as Critic The Turing Test and th.pdf:/Users/jfowers/Zotero/storage/7P9A8ZRQ/Ariza - 2009 - The Interrogator as Critic The Turing Test and th.pdf:application/pdf},
}

@misc{sturm_ai_2020,
	title = {{AI} {Music} {Generation} {Challenge} 2020},
	shorttitle = {ai-mus-gen},
	url = {https://boblsturm.github.io/aimusic2020/},
	abstract = {The 2020 Joint Conference on AI Music Creativity},
	language = {en-US},
	urldate = {2021-01-22},
	journal = {The 2020 Joint Conference on AI Music Creativity},
	author = {Sturm, Bob},
	year = {2020},
	file = {Snapshot:/Users/jfowers/Zotero/storage/QDD6ZJYX/aimusic2020.html:text/html},
}

@article{theis_note_2016,
	title = {A {Note} on the {Evaluation} of {Generative} {Models}},
	shorttitle = {eval-gen-models},
	url = {http://arxiv.org/abs/1511.01844},
	abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
	urldate = {2021-01-23},
	journal = {arXiv:1511.01844 [cs, stat]},
	author = {Theis, Lucas and Oord, Aäron van den and Bethge, Matthias},
	month = apr,
	year = {2016},
	note = {arXiv: 1511.01844},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jfowers/Zotero/storage/YET2NG64/Theis et al. - 2016 - A note on the evaluation of generative models.pdf:application/pdf;arXiv.org Snapshot:/Users/jfowers/Zotero/storage/K4K2KVMP/1511.html:text/html},
}

@misc{website_wmt_2020,
	title = {{EMNLP} 2020 {Fifth} {Conference} on {Machine} {Translation} ({WMT20})},
	shorttitle = {wmt2020},
	url = {http://www.statmt.org/wmt20/},
	urldate = {2021-01-23},
	journal = {2020 Fifth Conference on Machine Translation (WMT20)},
	author = {Haddow, Barry},
	year = {2020},
	file = {2020 Fifth Conference on Machine Translation (WMT20):/Users/jfowers/Zotero/storage/FRMG4F29/wmt20.html:text/html},
}

@inproceedings{cuthbert_music21_2010,
	title = {music21: {A} {Toolkit} for {Computer}-{Aided} {Musicology} and {Symbolic} {Music} {Data}},
	copyright = {Creative Commons Attribution-Noncommercial-Share Alike 3.0},
	shorttitle = {music21},
	url = {https://dspace.mit.edu/handle/1721.1/84963},
	abstract = {Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (score- based) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to provide powerful software tools integrated with sophisticated musical knowledge to both musicians with little programming experience (especially musicologists) and to programmers with only modest music theory skills. This paper introduces the music21 system, demonstrating how to use it and the types of problems it is well suited toward advancing. We include numerous examples of its power and flexibility, including demonstrations of graphing data and generating annotated musical scores.},
	language = {en\_US},
	urldate = {2021-01-23},
	booktitle = {Proceedings of the 11th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Cuthbert, Michael Scott and Ariza, Christopher},
	month = aug,
	year = {2010},
	note = {Accepted: 2014-02-14T18:40:17Z
ISBN: 9789039353813
Publisher: International Society for Music Information Retrieval},
	file = {Full Text PDF:/Users/jfowers/Zotero/storage/6QF6WXSN/Cuthbert and Ariza - 2010 - music21 A Toolkit for Computer-Aided Musicology a.pdf:application/pdf;Snapshot:/Users/jfowers/Zotero/storage/F47XVBE4/84963.html:text/html},
}

@misc{payne_musenet_2019,
	title = {{MuseNet}},
	shorttitle = {musenet},
	url = {https://openai.com/blog/musenet/},
	abstract = {We’ve created Musenet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments and can combine styles from country to Mozart to the Beatles.},
	language = {en},
	urldate = {2021-01-23},
	journal = {OpenAI},
	author = {Payne, Christine},
	month = apr,
	year = {2019},
	file = {Snapshot:/Users/jfowers/Zotero/storage/HRJK9MMB/musenet.html:text/html},
}

@inproceedings{johnson_generating_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Generating {Polyphonic} {Music} {Using} {Tied} {Parallel} {Networks}},
	isbn = {978-3-319-55750-2},
	shorttitle = {tied-paranets},
	doi = {10.1007/978-3-319-55750-2_9},
	abstract = {We describe a neural network architecture which enables prediction and composition of polyphonic music in a manner that preserves translation-invariance of the dataset. Specifically, we demonstrate training a probabilistic model of polyphonic music using a set of parallel, tied-weight recurrent networks, inspired by the structure of convolutional neural networks. This model is designed to be invariant to transpositions, but otherwise is intentionally given minimal information about the musical domain, and tasked with discovering patterns present in the source dataset. We present two versions of the model, denoted TP-LSTM-NADE and BALSTM, and also give methods for training the network and for generating novel music. This approach attains high performance at a musical prediction task and successfully creates note sequences which possess measure-level musical structure.},
	language = {en},
	booktitle = {Computational {Intelligence} in {Music}, {Sound}, {Art} and {Design}},
	publisher = {Springer International Publishing},
	author = {Johnson, Daniel D.},
	editor = {Correia, João and Ciesielski, Vic and Liapis, Antonios},
	year = {2017},
	keywords = {Convolutional Neural Network, Joint Probability Distribution, Prediction Task, Recurrent Neural Network, Translation Invariance},
	pages = {128--143},
}

@inproceedings{huang_music_2019,
	title = {Music {Transformer}: {Generating} {Music} with {Long}-{Term} {Structure}},
	shorttitle = {music-transformer},
	url = {https://openreview.net/pdf?id=rJe4ShAcF7},
	abstract = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions because their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modiﬁed relative attention mechanism can generate minute-long compositions (thousands of steps) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies1. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Maestro, and obtain state-of-the-art results on the latter.},
	language = {en},
	urldate = {2021-01-26},
	booktitle = {Proceedings of the 20th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Huang, Cheng-Zhi Anna and Uszkoreit, Ashish Vaswani Jakob and Shazeer, Noam and Hawthorne, Ian Simon Curtis and Dai, Andrew M and Hoffman, Matthew D and Eck, Monica Dinculescu Douglas},
	year = {2019},
	file = {Huang et al. - 2019 - MUSIC TRANSFORMER GENERATING MUSIC WITH LONG-TERM.pdf:/Users/jfowers/Zotero/storage/8ZVIMS5U/Huang et al. - 2019 - MUSIC TRANSFORMER GENERATING MUSIC WITH LONG-TERM.pdf:application/pdf},
}

@inproceedings{dong_muspy_2020,
	address = {Montreal, Canada},
	title = {{MusPy}: {A} {Toolkit} for {Symbolic} {Music} {Generation}},
	shorttitle = {muspy},
	url = {https://program.ismir2020.net/static/final_papers/187.pdf},
	abstract = {In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocessing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others—a process which is made easier by MusPy’s dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at https://github.com/salu133445/muspy.},
	language = {en},
	urldate = {2021-01-26},
	booktitle = {Proceedings of the 21st {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Dong, Hao-Wen and Chen, Ke and McAuley, Julian and Berg-Kirkpatrick, Taylor},
	year = {2020},
	file = {Dong et al. - 2020 - MUSPY A TOOLKIT FOR SYMBOLIC MUSIC GENERATION.pdf:/Users/jfowers/Zotero/storage/RIMBD8HS/Dong et al. - 2020 - MUSPY A TOOLKIT FOR SYMBOLIC MUSIC GENERATION.pdf:application/pdf},
}

@inproceedings{mcleod_midi_2020,
	address = {Montreal, Canada},
	title = {The {MIDI} {Degradation} {Toolkit}: {Symbolic} {Music} {Augmentation} and {Correction}},
	shorttitle = {mdtk},
	url = {https://program.ismir2020.net/static/final_papers/182.pdf},
	abstract = {In this paper, we introduce the MIDI Degradation Toolkit (MDTK), containing functions which take as input a musical excerpt (a set of notes with pitch, onset time, and duration), and return a “degraded” version of that excerpt with some error (or errors) introduced. Using the toolkit, we create the Altered and Corrupted MIDI Excerpts dataset version 1.0 (ACME v1.0), and propose four tasks of increasing difﬁculty to detect, classify, locate, and correct the degradations. We hypothesize that models trained for these tasks can be useful in (for example) improving automatic music transcription performance if applied as a post-processing step. To that end, MDTK includes a script that measures the distribution of different types of errors in a transcription, and creates a degraded dataset with similar properties. MDTK’s degradations can also be applied dynamically to a dataset during training (with or without the above script), generating novel degraded excerpts each epoch. MDTK could also be used to test the robustness of any system designed to take MIDI (or similar) data as input (e.g. systems designed for voice separation, metrical alignment, or chord detection) to such transcription errors or otherwise noisy data. The toolkit and dataset are both publicly available online, and we encourage contribution and feedback from the community.},
	language = {en},
	urldate = {2021-01-26},
	booktitle = {Proceedings of the 21st {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {McLeod, Andrew and Owers, James and Yoshii, Kazuyoshi},
	year = {2020},
	file = {McLeod et al. - 2020 - THE MIDI DEGRADATION TOOLKIT SYMBOLIC MUSIC AUGME.pdf:/Users/jfowers/Zotero/storage/AB7XKFB3/McLeod et al. - 2020 - THE MIDI DEGRADATION TOOLKIT SYMBOLIC MUSIC AUGME.pdf:application/pdf},
}
