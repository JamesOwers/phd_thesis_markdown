@article{ariza_interrogator_2009,
 author = {Ariza, Christopher},
 file = {Ariza - 2009 - The Interrogator as Critic The Turing Test and th.pdf:/Users/jfowers/Zotero/storage/7P9A8ZRQ/Ariza - 2009 - The Interrogator as Critic The Turing Test and th.pdf:application/pdf},
 issn = {0148-9267},
 journal = {Computer Music Journal},
 note = {Publisher: The MIT Press},
 number = {2},
 pages = {48--70},
 shorttitle = {critic},
 title = {The {Interrogator} as {Critic}: {The} {Turing} {Test} and the {Evaluation} of {Generative} {Music} {Systems}},
 url = {https://www.jstor.org/stable/40301027},
 urldate = {2021-01-22},
 volume = {33},
 year = {2009}
}

@article{briot_deep_2019,
 abstract = {This paper is a survey and an analysis of different ways of using deep learning (deep artificial neural networks) to generate musical content. We propose a methodology based on five dimensions for our analysis: Objective - What musical content is to be generated? Examples are: melody, polyphony, accompaniment or counterpoint. - For what destination and for what use? To be performed by a human(s) (in the case of a musical score), or by a machine (in the case of an audio file). Representation - What are the concepts to be manipulated? Examples are: waveform, spectrogram, note, chord, meter and beat. - What format is to be used? Examples are: MIDI, piano roll or text. - How will the representation be encoded? Examples are: scalar, one-hot or many-hot. Architecture - What type(s) of deep neural network is (are) to be used? Examples are: feedforward network, recurrent network, autoencoder or generative adversarial networks. Challenge - What are the limitations and open challenges? Examples are: variability, interactivity and creativity. Strategy - How do we model and control the process of generation? Examples are: single-step feedforward, iterative feedforward, sampling or input manipulation. For each dimension, we conduct a comparative analysis of various models and techniques and we propose some tentative multidimensional typology. This typology is bottom-up, based on the analysis of many existing deep-learning based systems for music generation selected from the relevant literature. These systems are described and are used to exemplify the various choices of objective, representation, architecture, challenge and strategy. The last section includes some discussion and some prospects.},
 author = {Briot, Jean-Pierre and Hadjeres, Gaëtan and Pachet, François-David},
 file = { BriotArXiv2018 Deep Learning Techniques for Music Generation – A Survey 2.pdf:/Users/jfowers/Zotero/storage/BRWXCHBW/ BriotArXiv2018 Deep Learning Techniques for Music Generation – A Survey 2.pdf:application/pdf;arXiv Fulltext PDF:/Users/jfowers/Zotero/storage/DL9MALUN/Briot et al. - 2019 - Deep Learning Techniques for Music Generation -- A.pdf:application/pdf;arXiv.org Snapshot:/Users/jfowers/Zotero/storage/WDPGAKY9/1709.html:text/html},
 journal = {arXiv:1709.01620 [cs]},
 keywords = {Computer Science - Machine Learning, Computer Science - Sound},
 note = {arXiv: 1709.01620},
 shorttitle = {dl4music-survey},
 title = {Deep {Learning} {Techniques} for {Music} {Generation} -- {A} {Survey}},
 url = {http://arxiv.org/abs/1709.01620},
 urldate = {2021-01-22},
 year = {2019}
}

@inproceedings{cuthbert_music21_2010,
 abstract = {Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (score- based) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to provide powerful software tools integrated with sophisticated musical knowledge to both musicians with little programming experience (especially musicologists) and to programmers with only modest music theory skills. This paper introduces the music21 system, demonstrating how to use it and the types of problems it is well suited toward advancing. We include numerous examples of its power and flexibility, including demonstrations of graphing data and generating annotated musical scores.},
 author = {Cuthbert, Michael Scott and Ariza, Christopher},
 booktitle = {Proceedings of the 11th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
 copyright = {Creative Commons Attribution-Noncommercial-Share Alike 3.0},
 file = {Full Text PDF:/Users/jfowers/Zotero/storage/6QF6WXSN/Cuthbert and Ariza - 2010 - music21 A Toolkit for Computer-Aided Musicology a.pdf:application/pdf;Snapshot:/Users/jfowers/Zotero/storage/F47XVBE4/84963.html:text/html},
 language = {en\_US},
 note = {Accepted: 2014-02-14T18:40:17Z
ISBN: 9789039353813
Publisher: International Society for Music Information Retrieval},
 shorttitle = {music21},
 title = {music21: {A} {Toolkit} for {Computer}-{Aided} {Musicology} and {Symbolic} {Music} {Data}},
 url = {https://dspace.mit.edu/handle/1721.1/84963},
 urldate = {2021-01-23},
 year = {2010}
}

@article{dhariwal_jukebox_2020,
 abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
 author = {Dhariwal, Prafulla* and Jun, Heewoo* and Payne, Christine* and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
 file = {arXiv.org Snapshot:/Users/jfowers/Zotero/storage/HWBR4YJX/2005.html:text/html;arXiv Fulltext PDF:/Users/jfowers/Zotero/storage/2FXBVPHZ/Dhariwal et al. - 2020 - Jukebox A Generative Model for Music.pdf:application/pdf},
 journal = {arXiv:2005.00341 [cs, eess, stat]},
 keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
 note = {arXiv: 2005.00341},
 shorttitle = {Jukebox},
 title = {Jukebox: {A} {Generative} {Model} for {Music}},
 url = {http://arxiv.org/abs/2005.00341},
 urldate = {2021-02-19},
 year = {2020}
}

@misc{dhariwal_jukebox_2020-1,
 abstract = {We’re introducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We’re releasing the model weights and code, along with a tool to explore the generated samples.},
 author = {Dhariwal, Prafulla* and Jun, Heewoo* and Payne, Christine* and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
 file = {Snapshot:/Users/jfowers/Zotero/storage/J8QEK43M/jukebox.html:text/html},
 journal = {OpenAI},
 language = {en},
 title = {Jukebox},
 url = {https://openai.com/blog/jukebox/},
 urldate = {2021-02-19},
 year = {2020}
}

@inproceedings{dong_muspy_2020,
 abstract = {In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocessing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others—a process which is made easier by MusPy’s dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at https://github.com/salu133445/muspy.},
 address = {Montreal, Canada},
 author = {Dong, Hao-Wen and Chen, Ke and McAuley, Julian and Berg-Kirkpatrick, Taylor},
 booktitle = {Proceedings of the 21st {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
 file = {Dong et al. - 2020 - MUSPY A TOOLKIT FOR SYMBOLIC MUSIC GENERATION.pdf:/Users/jfowers/Zotero/storage/RIMBD8HS/Dong et al. - 2020 - MUSPY A TOOLKIT FOR SYMBOLIC MUSIC GENERATION.pdf:application/pdf},
 language = {en},
 shorttitle = {muspy},
 title = {{MusPy}: {A} {Toolkit} for {Symbolic} {Music} {Generation}},
 url = {https://program.ismir2020.net/static/final_papers/187.pdf},
 urldate = {2021-01-26},
 year = {2020}
}

@misc{giraud_chapter_2015,
 abstract = {Can a computer understand musical forms? Musical forms describe how a piece of music is structured. They explain how the sections work together through repetition, contrast, and variation: repetition brings unity, and variation brings interest. Learning how to hear, to analyse, to play, or even to write music in various forms is part of music education. In this chapter, we briefly review some theories of musical form, and discuss the challenges of computational analysis of musical form. We discuss two sets of problems, segmentation and form analysis. We present studies in music information retrieval (MIR) related to both problems. Thinking about codification and automatic analysis of musical forms will help the development of better MIR algorithms.},
 author = {Giraud, M. and Groult, R. and Levé, F.},
 file = {Snapshot:/Users/jfowers/Zotero/storage/RQZBF49Q/800e6d00d48b57115bbf8c000a9f14b7048fdcf9.html:text/html},
 language = {en},
 title = {Chapter 5 {Computational} {Analysis} of {Musical} {Form}},
 url = {/paper/Chapter-5-Computational-Analysis-of-Musical-Form-Giraud-Groult/800e6d00d48b57115bbf8c000a9f14b7048fdcf9},
 urldate = {2021-02-17},
 year = {2015}
}

@misc{haddow_wmt_2020,
 author = {Haddow, Barry},
 file = {2020 Fifth Conference on Machine Translation (WMT20):/Users/jfowers/Zotero/storage/FRMG4F29/wmt20.html:text/html},
 journal = {2020 Fifth Conference on Machine Translation (WMT20)},
 shorttitle = {wmt2020},
 title = {{EMNLP} 2020 {Fifth} {Conference} on {Machine} {Translation} ({WMT20})},
 url = {http://www.statmt.org/wmt20/},
 urldate = {2021-01-23},
 year = {2020}
}

@book{hollings_ada_2018,
 abstract = {Ada, Countess of Lovelace (1815-1852), daughter of romantic poet Lord Byron and his highly educated wife, Anne Isabella, is sometimes called the world's first computer programmer and has become an icon for women in technology. But how did a young woman in the nineteenth century, without access to formal school or university education, acquire the knowledge and expertise to become a pioneer of computer science? Although an unusual pursuit for women at the time, Ada Lovelace studied science and mathematics from a young age. This book uses previously unpublished archival material to explore her precocious childhood, from her ideas for a steam-powered flying horse to penetrating questions about the science of rainbows. A remarkable correspondence course with the eminent mathematician Augustus De Morgan shows her developing into a gifted, perceptive and knowledgeable mathematician. Active in Victorian London's social and scientific elite alongside Mary Somerville, Michael Faraday and Charles Dickens, Ada Lovelace became fascinated by the computing machines devised by Charles Babbage. The table of mathematical formulae sometimes called the `first programme' occurs in her paper about his most ambitious invention, his unbuilt `Analytical Engine'.0Ada Lovelace died at just thirty-six, but her paper still strikes a chord to this day, with clear explanations of the principles of computing, and broader ideas on computer music and artificial intelligence now realised in modern digital computers. Featuring images of the `first programme' and Lovelace's correspondence, alongside mathematical models, and contemporary illustrations, this book shows how Ada Lovelace, with astonishing prescience, explored key mathematical questions to understand the principles behind modern computing},
 address = {Oxford},
 author = {Hollings, Christopher and Martin, Ursula and Rice, Adrian C.},
 isbn = {978-1-85124-488-1},
 keywords = {Biography, Computer programming, History, Lovelace, Ada King, Women computer programmers, Women in computer science, Women mathematicians},
 publisher = {Bodleian Library},
 shorttitle = {making-cs},
 title = {Ada {Lovelace}: {The} {Making} of a {Computer} {Scientist}},
 year = {2018}
}

@inproceedings{huang_music_2019,
 author = {Cheng{-}Zhi Anna Huang and
Ashish Vaswani and
Jakob Uszkoreit and
Ian Simon and
Curtis Hawthorne and
Noam Shazeer and
Andrew M. Dai and
Matthew D. Hoffman and
Monica Dinculescu and
Douglas Eck},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/HuangVUSHSDHDE19.bib},
 booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
New Orleans, LA, USA, May 6-9, 2019},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {Music Transformer: Generating Music with Long-Term Structure},
 url = {https://openreview.net/forum?id=rJe4ShAcF7},
 year = {2019}
}

@inproceedings{johnson_generating_2017,
 abstract = {We describe a neural network architecture which enables prediction and composition of polyphonic music in a manner that preserves translation-invariance of the dataset. Specifically, we demonstrate training a probabilistic model of polyphonic music using a set of parallel, tied-weight recurrent networks, inspired by the structure of convolutional neural networks. This model is designed to be invariant to transpositions, but otherwise is intentionally given minimal information about the musical domain, and tasked with discovering patterns present in the source dataset. We present two versions of the model, denoted TP-LSTM-NADE and BALSTM, and also give methods for training the network and for generating novel music. This approach attains high performance at a musical prediction task and successfully creates note sequences which possess measure-level musical structure.},
 address = {Cham},
 author = {Johnson, Daniel D.},
 booktitle = {Computational {Intelligence} in {Music}, {Sound}, {Art} and {Design}},
 doi = {10.1007/978-3-319-55750-2_9},
 editor = {Correia, João and Ciesielski, Vic and Liapis, Antonios},
 isbn = {978-3-319-55750-2},
 keywords = {Convolutional Neural Network, Joint Probability Distribution, Prediction Task, Recurrent Neural Network, Translation Invariance},
 language = {en},
 pages = {128--143},
 publisher = {Springer International Publishing},
 series = {Lecture {Notes} in {Computer} {Science}},
 shorttitle = {tied-paranets},
 title = {Generating {Polyphonic} {Music} {Using} {Tied} {Parallel} {Networks}},
 year = {2017}
}

@incollection{marsden_music_2016,
 abstract = {This chapter examines questions of what is to be analysed in computational music analysis, what is to be produced, and how one can have confidence in the results. These are not new issues for music analysis, but their consequences are here considered explicitly from the perspective of computational analysis. Music analysis without computers is able to operate with multiple or even indistinct conceptions of the material to be analysed because it can use multiple references whose meanings shift from context to context. Computational analysis, by contrast, must operate with definite inputs and produce definite outputs. Computational analysts must therefore face the issues of error and approximation explicitly. While computational analysis must retain contact with music analysis as it is generally practised, I argue that the most promising approach for the development of computational analysis is not systems to mimic human analysis, but instead systems to answer specific music-analytical questions. The chapter concludes with several consequent recommendations for future directions in computational music analysis.},
 address = {Cham},
 author = {Marsden, Alan},
 booktitle = {Computational {Music} {Analysis}},
 doi = {10.1007/978-3-319-25931-4_1},
 editor = {Meredith, David},
 isbn = {978-3-319-25931-4},
 keywords = {Computational Analysis, Kolmogorov Complexity, Music Information Retrieval, Music Notation, Music Perception},
 language = {en},
 pages = {3--28},
 publisher = {Springer International Publishing},
 shorttitle = {Music {Analysis} by {Computer}},
 title = {Music {Analysis} by {Computer}: {Ontology} and {Epistemology}},
 url = {https://doi.org/10.1007/978-3-319-25931-4_1},
 urldate = {2021-02-17},
 year = {2016}
}

@inproceedings{mcleod_midi_2020,
 abstract = {In this paper, we introduce the MIDI Degradation Toolkit (MDTK), containing functions which take as input a musical excerpt (a set of notes with pitch, onset time, and duration), and return a “degraded” version of that excerpt with some error (or errors) introduced. Using the toolkit, we create the Altered and Corrupted MIDI Excerpts dataset version 1.0 (ACME v1.0), and propose four tasks of increasing difﬁculty to detect, classify, locate, and correct the degradations. We hypothesize that models trained for these tasks can be useful in (for example) improving automatic music transcription performance if applied as a post-processing step. To that end, MDTK includes a script that measures the distribution of different types of errors in a transcription, and creates a degraded dataset with similar properties. MDTK’s degradations can also be applied dynamically to a dataset during training (with or without the above script), generating novel degraded excerpts each epoch. MDTK could also be used to test the robustness of any system designed to take MIDI (or similar) data as input (e.g. systems designed for voice separation, metrical alignment, or chord detection) to such transcription errors or otherwise noisy data. The toolkit and dataset are both publicly available online, and we encourage contribution and feedback from the community.},
 address = {Montreal, Canada},
 author = {McLeod, Andrew and Owers, James and Yoshii, Kazuyoshi},
 booktitle = {Proceedings of the 21st {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
 file = {McLeod et al. - 2020 - THE MIDI DEGRADATION TOOLKIT SYMBOLIC MUSIC AUGME.pdf:/Users/jfowers/Zotero/storage/AB7XKFB3/McLeod et al. - 2020 - THE MIDI DEGRADATION TOOLKIT SYMBOLIC MUSIC AUGME.pdf:application/pdf},
 language = {en},
 shorttitle = {mdtk},
 title = {The {MIDI} {Degradation} {Toolkit}: {Symbolic} {Music} {Augmentation} and {Correction}},
 url = {https://program.ismir2020.net/static/final_papers/182.pdf},
 urldate = {2021-01-26},
 year = {2020}
}

@misc{payne_musenet_2019,
 abstract = {We’ve created Musenet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments and can combine styles from country to Mozart to the Beatles.},
 author = {Payne, Christine},
 file = {Snapshot:/Users/jfowers/Zotero/storage/HRJK9MMB/musenet.html:text/html},
 journal = {OpenAI},
 language = {en},
 shorttitle = {musenet},
 title = {{MuseNet}},
 url = {https://openai.com/blog/musenet/},
 urldate = {2021-01-23},
 year = {2019}
}

@misc{sturm_ai_2020,
 abstract = {The 2020 Joint Conference on AI Music Creativity},
 author = {Sturm, Bob},
 file = {Snapshot:/Users/jfowers/Zotero/storage/QDD6ZJYX/aimusic2020.html:text/html},
 journal = {The 2020 Joint Conference on AI Music Creativity},
 language = {en-US},
 shorttitle = {ai-mus-gen},
 title = {{AI} {Music} {Generation} {Challenge} 2020},
 url = {https://boblsturm.github.io/aimusic2020/},
 urldate = {2021-01-22},
 year = {2020}
}

@misc{sturm_benchmarking_2017,
 abstract = {A recent conversation on the Google magenta project is about benchmarking “music generation systems” like David Cope’s Experiments in Music Intelligence, magenta’s own recur…},
 author = {Sturm, Bob L. T.},
 file = {Snapshot:/Users/jfowers/Zotero/storage/2EZUFWII/benchmarking-music-generation-systems.html:text/html},
 journal = {Folk the Algorithms},
 language = {en},
 title = {Benchmarking “music generation systems”?},
 url = {https://highnoongmt.wordpress.com/2017/03/19/benchmarking-music-generation-systems/},
 urldate = {2021-02-18},
 year = {2017}
}

@inproceedings{theis_note_2016,
 author = {Lucas Theis and
A{\"{a}}ron van den Oord and
Matthias Bethge},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/TheisOB15.bib},
 booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
 editor = {Yoshua Bengio and
Yann LeCun},
 timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
 title = {A note on the evaluation of generative models},
 url = {http://arxiv.org/abs/1511.01844},
 year = {2016}
}

