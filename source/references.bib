
@article{savage_statistical_2015,
	title = {Statistical universals reveal the structures and functions of human music},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1414495112},
	doi = {10.1073/pnas.1414495112},
	abstract = {Music has been called “the universal language of mankind.” Although contemporary theories of music evolution often invoke various musical universals, the existence of such universals has been disputed for decades and has never been empirically demonstrated. Here we combine a music-classification scheme with statistical analyses, including phylogenetic comparative methods, to examine a well-sampled global set of 304 music recordings. Our analyses reveal no absolute universals but strong support for many statistical universals that are consistent across all nine geographic regions sampled. These universals include 18 musical features that are common individually as well as a network of 10 features that are commonly associated with one another. They span not only features related to pitch and rhythm that are often cited as putative universals but also rarely cited domains including performance style and social context. These cross-cultural structural regularities of human music may relate to roles in facilitating group coordination and cohesion, as exemplified by the universal tendency to sing, play percussion instruments, and dance to simple, repetitive music in groups. Our findings highlight the need for scientists studying music evolution to expand the range of musical cultures and musical features under consideration. The statistical universals we identified represent important candidates for future investigation.},
	language = {en},
	number = {29},
	urldate = {2021-01-19},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Savage, Patrick E. and Brown, Steven and Sakai, Emi and Currie, Thomas E.},
	month = jul,
	year = {2015},
	pages = {8987--8992},
	file = { SavagePNAS2015 Statistical universals reveal the structures and functions of human music.pdf:/Users/jfowers/Dropbox/Notability/Reading REFERENCE/ SavagePNAS2015 Statistical universals reveal the structures and functions of human music.pdf:application/pdf},
}

@inproceedings{jeong_graph_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Graph {Neural} {Network} for {Music} {Score} {Data} and {Modeling} {Expressive} {Piano} {Performance}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/jeong19a.html},
	abstract = {Music score is often handled as one-dimensional sequential data. Unlike words in a text document, notes in music score can be played simultaneously by the polyphonic nature and each of them has its own duration. In this paper, we represent the unique form of musical score using graph neural network and apply it for rendering expressive piano performance from the music score. Specifically, we design the model using note-level gated graph neural network and measure-level hierarchical attention network with bidirectional long short-term memory with an iterative feedback method. In addition, to model different styles of performance for a given input score, we employ a variational auto-encoder. The result of the listening test shows that our proposed model generated more human-like performances compared to a baseline model and a hierarchical attention network model that handles music score as a word-like sequence.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jeong, Dasaem and Kwon, Taegyun and Kim, Yoojin and Nam, Juhan},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	pages = {3060--3070},
	file = {Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance.pdf:/Users/jfowers/Dropbox/Notability/Reading REFERENCE/ JeongJMLR2019 Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance.pdf:application/pdf},
}
